---
title: å¤§æ•°æ®é¢è¯•é¢˜100é“
date: 2021-08-25 12:50:07
permalink: /pages/5ff074/
categories:
  - æ›´å¤š
  - é¢è¯•
tags:
  - 
---

# å¤§æ•°æ®å·¥ç¨‹å¸ˆé¢è¯•é¢˜

## é€‰æ‹©é¢˜

1. ä¸‹é¢å“ªä¸ªç¨‹åºè´Ÿè´£HDFSæ•°æ®å­˜å‚¨ã€‚

   a) NameNode 	b) Jobtracker 	c) Datanode 	d) secondaryNameNode 	e) tasktracker 

   ç­”æ¡ˆ C datanode

   

2. HDfS ä¸­çš„ block é»˜è®¤ä¿å­˜å‡ ä»½ï¼Ÿ 

   a) 3 ä»½ 	b) 2 ä»½ 	c) 1 ä»½ 	d) ä¸ç¡®å®š 

   ç­”æ¡ˆ A é»˜è®¤ 3 ä»½

   

3. ä¸‹åˆ—å“ªä¸ªç¨‹åºé€šå¸¸ä¸ NameNode åœ¨ä¸€ä¸ªèŠ‚ç‚¹å¯åŠ¨? 

   a) SecondaryNameNode 	b) DataNode 	c) TaskTracker 	d) Jobtracker 

   ç­”æ¡ˆ D

   

4. HDFS é»˜è®¤ Block Size 

   a) 32MB	b) 64MB	c) 128MB 

   ç­”æ¡ˆï¼šB

   

5. ä¸‹åˆ—å“ªé¡¹é€šå¸¸æ˜¯é›†ç¾¤çš„æœ€ä¸»è¦ç“¶é¢ˆ 

   a) CPU 	b) ç½‘ç»œ 	c) ç£ç›˜ IO 	d) å†…å­˜ 

   ç­”æ¡ˆï¼šC ç£ç›˜

> é¦–å…ˆé›†ç¾¤çš„ç›®çš„æ˜¯ä¸ºäº†èŠ‚çœæˆæœ¬ï¼Œç”¨å»‰ä»·çš„ pc æœºï¼Œå–ä»£å°å‹æœºåŠå¤§å‹æœºã€‚å°å‹æœºå’Œå¤§å‹æœº æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ 
>
> 1.cpu å¤„ç†èƒ½åŠ›å¼º 
>
> 2.å†…å­˜å¤Ÿå¤§ï¼Œæ‰€ä»¥é›†ç¾¤çš„ç“¶é¢ˆä¸å¯èƒ½æ˜¯ a å’Œ d 
>
> 3.å¦‚æœæ˜¯äº’è”ç½‘æœ‰ç“¶é¢ˆï¼Œå¯ä»¥è®©é›†ç¾¤æ­å»ºå†…ç½‘ã€‚æ¯æ¬¡å†™å…¥æ•°æ®éƒ½è¦é€šè¿‡ç½‘ç»œï¼ˆé›†ç¾¤æ˜¯å†…ç½‘ï¼‰ï¼Œ ç„¶åè¿˜è¦å†™å…¥ 3 ä»½æ•°æ®ï¼Œæ‰€ä»¥ IO å°±ä¼šæ‰“æŠ˜æ‰£ã€‚



6. å…³äº SecondaryNameNode å“ªé¡¹æ˜¯æ­£ç¡®çš„ï¼Ÿ

   a) å®ƒæ˜¯ NameNode çš„çƒ­å¤‡ 	b) å®ƒå¯¹å†…å­˜æ²¡æœ‰è¦æ±‚ 	c) å®ƒçš„ç›®çš„æ˜¯å¸®åŠ© NameNode åˆå¹¶ç¼–è¾‘æ—¥å¿—ï¼Œå‡å°‘ NameNode å¯åŠ¨æ—¶é—´ 	d) SecondaryNameNode åº”ä¸ NameNode éƒ¨ç½²åˆ°ä¸€ä¸ªèŠ‚ç‚¹ 

   ç­”æ¡ˆ C

   

7. ä¸‹åˆ—å“ªé¡¹å¯ä»¥ä½œä¸ºé›†ç¾¤çš„ç®¡ç†ï¼Ÿ 

   a) Puppet 	b) Pdsh 	c) Cloudera Manager d) Zookeeper 

   ç­”æ¡ˆ ABD 	å…·ä½“å¯æŸ¥çœ‹ä»€ä¹ˆæ˜¯ Zookeeperï¼ŒZookeeper çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Œåœ¨ Hadoop åŠ hbase ä¸­å…·ä½“ä½œ ç”¨æ˜¯ä»€ä¹ˆã€‚

   

8. Client ç«¯ä¸Šä¼ æ–‡ä»¶çš„æ—¶å€™ä¸‹åˆ—å“ªé¡¹æ­£ç¡® 

   a) æ•°æ®ç»è¿‡ NameNode ä¼ é€’ç»™ DataNode 	b) Client ç«¯å°†æ–‡ä»¶åˆ‡åˆ†ä¸º Blockï¼Œä¾æ¬¡ä¸Šä¼  	c) Client åªä¸Šä¼ æ•°æ®åˆ°ä¸€å° DataNodeï¼Œç„¶åç”± NameNode è´Ÿè´£ Block å¤åˆ¶å·¥ä½œ 

   ç­”æ¡ˆ B åˆ†æï¼šClient å‘ NameNode å‘èµ·æ–‡ä»¶å†™å…¥çš„è¯·æ±‚ã€‚NameNode æ ¹æ®æ–‡ä»¶å¤§å°å’Œæ–‡ä»¶å—é…ç½® æƒ…å†µï¼Œè¿”å›ç»™ Client å®ƒæ‰€ç®¡ç†éƒ¨åˆ† DataNode çš„ä¿¡æ¯ã€‚Client å°†æ–‡ä»¶åˆ’åˆ†ä¸ºå¤šä¸ª Blockï¼Œæ ¹æ® DataNode çš„åœ°å€ä¿¡æ¯ï¼ŒæŒ‰é¡ºåºå†™å…¥åˆ°æ¯ä¸€ä¸ª DataNode å—ä¸­ã€‚å…·ä½“æŸ¥çœ‹ HDFS ä½“ç³»ç»“æ„ç®€ä»‹åŠä¼˜ç¼ºç‚¹ã€‚



9. ä¸‹åˆ—å“ªä¸ªæ˜¯ Hadoop è¿è¡Œçš„æ¨¡å¼ 

   a) å•æœºç‰ˆ 	b) ä¼ªåˆ†å¸ƒå¼ 	c) åˆ†å¸ƒå¼ 

   ç­”æ¡ˆ ABC å•æœºç‰ˆ,ä¼ªåˆ†å¸ƒå¼åªæ˜¯å­¦ä¹ ç”¨çš„ã€‚

---

## é¢è¯•é¢˜

### Hadoop çš„æ ¸å¿ƒé…ç½®æ˜¯ä»€ä¹ˆï¼Ÿ 

Hadoop çš„æ ¸å¿ƒé…ç½®é€šè¿‡ä¸¤ä¸ª xml æ–‡ä»¶æ¥å®Œæˆï¼š1ï¼Œhadoop-default.xmlï¼›2ï¼Œhadoop-site.xmlã€‚ è¿™äº›æ–‡ä»¶éƒ½ä½¿ç”¨ xml æ ¼å¼ï¼Œå› æ­¤æ¯ä¸ª xml ä¸­éƒ½æœ‰ä¸€äº›å±æ€§ï¼ŒåŒ…æ‹¬åç§°å’Œå€¼ï¼Œä½†æ˜¯å½“ä¸‹è¿™äº›æ–‡ ä»¶éƒ½å·²ä¸å¤å­˜åœ¨ã€‚



### é‚£å½“ä¸‹åˆè¯¥å¦‚ä½•é…ç½®ï¼Ÿ

Hadoop ç°åœ¨æ‹¥æœ‰ 3 ä¸ªé…ç½®æ–‡ä»¶ï¼š1ï¼Œcore-site.xmlï¼›2ï¼Œhdfs-site.xmlï¼›3ï¼Œmapred-site.xmlã€‚è¿™ äº›æ–‡ä»¶éƒ½ä¿å­˜åœ¨ conf/å­ç›®å½•ä¸‹ã€‚



### â€œjpsâ€å‘½ä»¤çš„ç”¨å¤„ï¼Ÿ

 è¿™ä¸ªå‘½ä»¤å¯ä»¥æ£€æŸ¥ Namenodeã€Datanodeã€Task Trackerã€ Job Tracker æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚



### â¤ï¸mapreduce çš„åŸç†?

![image-20210825130550811](https://gitee.com/vicxsl/img/raw/master/img/1629867951423/image-20210825130550811.png)

1. å•è¡Œæ–‡ä»¶å†…å®¹ä½œä¸ºè¾“å…¥
2. åˆ†å¸ƒå¼çš„DataNodeå¤„ç†æ•°æ®
   1. é€šè¿‡Mapperå¤„ç†æ–‡ä»¶ï¼Œå¤„ç†åç§»é‡å’Œæ•°æ®åˆ†å‰²
   2. content å¾ªç¯è¾“å‡ºåˆ†å‰²å¥½çš„å†…å®¹
3. æ‹·è´æ•°æ®åˆ°å…¶ä»–DataNodeï¼ˆåˆ†åŒºå¥½ä¹‹åï¼Ÿï¼‰
4. Reducerå¤„ç†ï¼ŒåŠ å·¥åè¾“å‡ºæ•°æ®

### HDFS å­˜å‚¨çš„æœºåˆ¶?

### HDFS å†™æµç¨‹

![image-20210825132443536](https://gitee.com/vicxsl/img/raw/master/img/1629869083912/image-20210825132443536.png)

æµç¨‹ï¼š 

1.  client é“¾æ¥ namenode å­˜æ•°æ® 

2. namenode è®°å½•ä¸€æ¡æ•°æ®ä½ç½®ä¿¡æ¯ï¼ˆå…ƒæ•°æ®ï¼‰ï¼Œå‘Šè¯‰ client å­˜å“ªã€‚ 

3. client ç”¨ hdfs çš„ api å°†æ•°æ®å—ï¼ˆé»˜è®¤æ˜¯ 64Mï¼‰å­˜å‚¨åˆ° datanode ä¸Šã€‚ 

4.  datanode å°†æ•°æ®æ°´å¹³å¤‡ä»½ã€‚å¹¶ä¸”å¤‡ä»½å®Œå°†åé¦ˆ clientã€‚ 

5. client é€šçŸ¥ namenode å­˜å‚¨å—å®Œæ¯•ã€‚ 

6. namenode å°†å…ƒæ•°æ®åŒæ­¥åˆ°å†…å­˜ä¸­ã€‚ 

7. å¦ä¸€å—å¾ªç¯ä¸Šé¢çš„è¿‡ç¨‹ã€‚

   

### HDFSè¯»æµç¨‹

![image-20210825132702811](https://gitee.com/vicxsl/img/raw/master/img/1629869223178/image-20210825132702811.png)

æµç¨‹ï¼š

1. client é“¾æ¥ namenodeï¼ŒæŸ¥çœ‹å…ƒæ•°æ®ï¼Œæ‰¾åˆ°æ•°æ®çš„å­˜å‚¨ä½ç½®ã€‚ 2. 
2. client é€šè¿‡ hdfs çš„ api å¹¶å‘è¯»å–æ•°æ®ã€‚ 
3. å…³é—­è¿æ¥ã€‚



### ä¸¾ä¸€ä¸ªç®€å•çš„ä¾‹å­è¯´æ˜ mapreduce æ˜¯æ€ä¹ˆæ¥è¿è¡Œçš„ ?

wordcount çš„ä¾‹å­



### ç”¨ mapreduce æ¥å®ç°ä¸‹é¢éœ€æ±‚ï¼Ÿ 

ç°åœ¨æœ‰ 10 ä¸ªæ–‡ä»¶å¤¹,æ¯ä¸ªæ–‡ä»¶å¤¹éƒ½æœ‰ 1000000 ä¸ª url.ç°åœ¨è®©ä½ æ‰¾å‡º top1000000urlã€‚ 

è§£ç­”ï¼štopk



(è¿˜å¯ä»¥ç”¨ treeMap, åˆ° 1000000 äº†æ¯æ¥ä¸€ä¸ªéƒ½åŠ è¿›å», åˆ æ‰æœ€å°çš„)



### hadoop ä¸­ Combiner çš„ä½œç”¨?

combiner æ˜¯ reduce çš„å®ç°ï¼Œåœ¨ map ç«¯è¿è¡Œè®¡ç®—ä»»åŠ¡ï¼Œå‡å°‘ map ç«¯çš„è¾“å‡ºæ•°æ®ã€‚ ä½œç”¨å°±æ˜¯ä¼˜åŒ–ã€‚ 

ä½†æ˜¯ combiner çš„ä½¿ç”¨åœºæ™¯æ˜¯ mapreduce çš„ map å’Œ reduce è¾“å…¥è¾“å‡ºä¸€æ ·ã€‚



### ç®€è¿° hadoop å®‰è£…

![image-20210825133000964](https://gitee.com/vicxsl/img/raw/master/img/1629869401349/image-20210825133000964.png)

1. åˆ›å»º hadoop å¸æˆ·.
2. setup.æ”¹IP.
3. å®‰è£…javaï¼Œå¹¶ä¿®æ”¹/etc/profile æ–‡ä»¶ï¼Œé…ç½® java çš„ç¯å¢ƒå˜é‡.
4. ä¿®æ”¹ Host æ–‡ä»¶åŸŸå.
5. å®‰è£… SSHï¼Œé…ç½®æ— å¯†é’¥é€šä¿¡.
6. è§£å‹hadoop.
7. é…ç½® conf æ–‡ä»¶ä¸‹ hadoop-env.shã€core-site.shã€ mapre-site.shã€hdfs-site.sh.
8. é…ç½®hadoopçš„ç¯å¢ƒå˜é‡,
9. Hadoop namenode -format
10. Start-all



### è¯·åˆ—å‡º hadoop è¿›ç¨‹å

![image-20210825133236037](https://gitee.com/vicxsl/img/raw/master/img/1629869556412/image-20210825133236037.png)

1. namenode : ç®¡ç†é›†ç¾¤ï¼Œå¹¶è®°å½• datanode æ–‡ä»¶ä¿¡æ¯.
2. Secondname:å¯ä»¥åšå†·å¤‡ï¼Œå¯¹ä¸€å®šèŒƒå›´å†…æ•°æ®åšå¿«ç…§æ€§å¤‡ä»½.
3. Datanode:å­˜å‚¨æ•°æ®
4. Jobtracker :ç®¡ç†ä»»åŠ¡ï¼Œå¹¶å°†ä»»åŠ¡åˆ†é…ç»™ tasktracker.
5. Tasktracker:ä»»åŠ¡æ‰§è¡Œæ–¹.



### è§£å†³ä¸‹é¢çš„é”™è¯¯

![image-20210825133409119](https://gitee.com/vicxsl/img/raw/master/img/1629869649566/image-20210825133409119.png)

1ã€ æƒé™é—®é¢˜ï¼Œå¯èƒ½æ›¾ç»ç”¨ rootå¯åŠ¨è¿‡é›†ç¾¤ã€‚ ( ä¾‹å¦‚ hadoopæ­å»ºçš„é›†ç¾¤ ,æ˜¯tmp/hadoop hadoop/..... )
2ã€ å¯èƒ½æ˜¯æ–‡ä»¶å¤¹ä¸å­˜åœ¨( Directory does not exist )
3ã€ è§£å†³ : åˆ æ‰ tmpä¸‹çš„é‚£ä¸ªæ–‡ä»¶ ,æˆ–æ”¹æˆå½“å‰ç”¨æˆ·ï¼ˆ Directory /tmp/hadoop-root/ ï¼‰



### å†™å‡ºä¸‹é¢çš„å‘½ä»¤

![image-20210825133650988](https://gitee.com/vicxsl/img/raw/master/img/1629869811385/image-20210825133650988.png)

1. hadoop job -list 

2. æ‹¿åˆ°job-id  shadoop job -kill job-id

3. Hadoop fs -rmr /tmp/aaa

4. åŠ æ–°èŠ‚ç‚¹æ—¶:
   Hadoop-daemon.sh start datanode

   Hadoop-daemon.sh start tasktracker

5. åˆ é™¤æ—¶:

   Hadoop mradmin -refreshnodes
   Hadoop dfsadmin -refreshnodes



### ç®€è¿° hadoopçš„è°ƒåº¦å™¨

![image-20210826131756492](https://gitee.com/vicxsl/img/raw/master/img/1629955077154/image-20210826131756492.png)

Fifo schedular : é»˜è®¤ï¼Œå…ˆè¿›å…ˆå‡ºçš„åŸåˆ™ã€‚
Capacity schedular : è®¡ç®—èƒ½åŠ›è°ƒåº¦å™¨ï¼Œé€‰æ‹©å ç”¨æœ€å°ã€ä¼˜å…ˆçº§é«˜çš„å…ˆæ‰§è¡Œï¼Œä¾æ­¤ç±»æ¨ã€‚
Fair schedular: å…¬å¹³è°ƒåº¦ï¼Œæ‰€æœ‰çš„ job å…·æœ‰ç›¸åŒçš„èµ„æºã€‚



### åˆ—å‡ºä½ å¼€å‘ mapreduceçš„è¯­è¨€

![image-20210826131927559](https://gitee.com/vicxsl/img/raw/master/img/1629955167927/image-20210826131927559.png)

java



### ä¹¦å†™ç¨‹åº

![image-20210826132034992](https://gitee.com/vicxsl/img/raw/master/img/1629955235358/image-20210826132034992.png)

wordcount



### ä¸åŒè¯­è¨€çš„ä¼˜ç¼ºç‚¹

![image-20210826132057466](https://gitee.com/vicxsl/img/raw/master/img/1629955257822/image-20210826132057466.png)

hadoop

hadoopæ˜¯ javaå†™çš„ï¼Œ javaçš„é›†æˆæ•ˆæœæœ€å¥½ï¼Œå¹¶ä¸”å¹³å°ç¯å¢ƒç»Ÿä¸€ã€‚



### hiveæœ‰å“ªäº›ä¿å­˜å…ƒæ•°æ®çš„æ–¹å¼ï¼Œä¸ªæœ‰ä»€ä¹ˆç‰¹ç‚¹ã€‚

![image-20210826132128077](https://gitee.com/vicxsl/img/raw/master/img/1629955288440/image-20210826132128077.png)

1ã€ å†…å­˜æ•°æ®åº“ derbyï¼Œå®‰è£…å°ï¼Œä½†æ˜¯æ•°æ®å­˜åœ¨å†…å­˜ï¼Œä¸ç¨³å®š
2ã€ mysqlæ•°æ®åº“ï¼Œæ•°æ®å­˜å‚¨æ¨¡å¼å¯ä»¥è‡ªå·±è®¾ç½®ï¼ŒæŒä¹…åŒ–å¥½ï¼ŒæŸ¥çœ‹æ–¹ä¾¿ã€‚



combinerå’Œ partitionçš„ä½œç”¨

![image-20210826132158158](https://gitee.com/vicxsl/img/raw/master/img/1629955318528/image-20210826132158158.png)

combineræ˜¯ reduceçš„å®ç°ï¼Œåœ¨ mapç«¯è¿è¡Œè®¡ç®—ä»»åŠ¡ï¼Œå‡å°‘ mapç«¯çš„è¾“å‡ºæ•°æ®ã€‚
**ä½œç”¨å°±æ˜¯ä¼˜åŒ–ã€‚**
ä½†æ˜¯combinerçš„ä½¿ç”¨åœºæ™¯æ˜¯mapreduceçš„ mapè¾“å‡ºç»“æœå’Œ reduceè¾“å…¥è¾“å‡ºä¸€æ ·ã€‚



**partitionçš„é»˜è®¤å®ç°æ˜¯ hashpartitionï¼Œæ˜¯ mapç«¯å°†æ•°æ®æŒ‰ç…§ reduceä¸ªæ•°å–ä½™**ï¼Œè¿›è¡Œåˆ†åŒºï¼Œ
ä¸åŒçš„ reduceæ¥ copyè‡ªå·±çš„æ•°æ®ã€‚
partitionçš„ä½œç”¨æ˜¯å°†æ•°æ®åˆ†åˆ°ä¸åŒçš„ reduceè¿›è¡Œè®¡ç®—ï¼ŒåŠ å¿«è®¡ç®—æ•ˆæœã€‚



### hiveå†…éƒ¨è¡¨å’Œå¤–éƒ¨è¡¨çš„åŒºåˆ«

å†…éƒ¨è¡¨ï¼šåŠ è½½æ•°æ®åˆ° hiveæ‰€åœ¨çš„ hdfsç›®å½•ï¼Œ åˆ é™¤æ—¶ï¼Œå…ƒæ•°æ®å’Œæ•°æ®æ–‡ä»¶éƒ½åˆ é™¤ã€‚
å¤–éƒ¨è¡¨ï¼šä¸åŠ è½½æ•°æ®åˆ° hiveæ‰€åœ¨çš„ hdfsç›®å½•ï¼Œåˆ é™¤æ—¶ï¼Œåªåˆ é™¤è¡¨ç»“æ„ã€‚



### hbaseçš„ rowkeyæ€ä¹ˆåˆ›å»ºå¥½ï¼Ÿåˆ—æ—æ€ä¹ˆåˆ›å»ºæ¯”è¾ƒå¥½ï¼Ÿ

hbaseå­˜å‚¨æ—¶ï¼Œæ•°æ®æŒ‰ç…§ Row keyçš„å­—å…¸åº (byte order)æ’åºå­˜å‚¨ã€‚è®¾è®¡ keyæ—¶ï¼Œè¦å……åˆ†æ’åº
å­˜å‚¨è¿™ä¸ªç‰¹æ€§ï¼Œå°†ç»å¸¸ä¸€èµ·è¯»å–çš„è¡Œå­˜å‚¨æ”¾åˆ°ä¸€èµ·ã€‚ (ä½ç½®ç›¸å…³æ€§ )

ä¸€ä¸ªåˆ—æ—åœ¨æ•°æ®åº•å±‚æ˜¯ä¸€ä¸ªæ–‡ä»¶ï¼Œæ‰€ä»¥å°†ç»å¸¸ä¸€èµ·æŸ¥è¯¢çš„åˆ—æ”¾åˆ°ä¸€ä¸ªåˆ—æ—ä¸­ï¼Œåˆ—æ—å°½é‡å°‘ï¼Œå‡å°‘æ–‡ä»¶çš„å¯»å€æ—¶é—´ã€‚å‡å°‘æ–‡ä»¶çš„å¯»å€æ—¶é—´ã€‚



### â¤ï¸ç”¨ mapreduceæ€ä¹ˆå¤„ç†æ•°æ®å€¾æ–œé—®é¢˜ï¼Ÿ

æ•°æ®å€¾æ–œï¼š

map /reduceç¨‹åºæ‰§è¡Œæ—¶ï¼Œ reduceèŠ‚ç‚¹å¤§éƒ¨åˆ†æ‰§è¡Œå®Œæ¯•ï¼Œä½†æ˜¯æœ‰ä¸€ä¸ªæˆ–è€…å‡ ä¸ªreduceèŠ‚ç‚¹è¿è¡Œå¾ˆæ…¢ï¼Œå¯¼è‡´æ•´ä¸ªç¨‹åºçš„å¤„ç†æ—¶é—´å¾ˆé•¿ï¼Œè¿™æ˜¯å› ä¸ºæŸä¸€ä¸ª keyçš„æ¡æ•°æ¯”å…¶ä»–keyå¤šå¾ˆå¤šï¼ˆæœ‰æ—¶æ˜¯ç™¾å€æˆ–è€…åƒå€ä¹‹å¤šï¼‰ï¼Œè¿™æ¡ keyæ‰€åœ¨çš„ reduceèŠ‚ç‚¹æ‰€å¤„ç†çš„æ•°æ®é‡æ¯”å…¶ä»–èŠ‚ç‚¹å°±å¤§å¾ˆå¤šï¼Œä»è€Œå¯¼è‡´æŸå‡ ä¸ªèŠ‚ç‚¹è¿Ÿè¿Ÿè¿è¡Œä¸å®Œï¼Œæ­¤ç§°ä¹‹ä¸ºæ•°æ®å€¾æ–œã€‚
ç”¨hadoopç¨‹åºè¿›è¡Œæ•°æ®å…³è”æ—¶ï¼Œå¸¸ç¢°åˆ°æ•°æ®å€¾æ–œçš„æƒ…å†µï¼Œè¿™é‡Œæä¾›ä¸€ç§è§£å†³æ–¹æ³•ã€‚

è‡ªå·±å®ç°**partitionç±»ï¼Œç”¨** **keyå’Œ valueç›¸åŠ å– hashå€¼**ï¼š

æ–¹å¼1ï¼š
æºä»£ç ï¼š

```java
public int getPartition(K key, V value,int numReduceTasks) 
{ 
    return (key.hashCode() & Integer.MAX_VALUE) % numReduceTasks;
}

ä¿®æ”¹å
public int getPartition(K key, V value,int numReduceTasks) 
{ 
    return ((ï¼ˆkey).hashCode()+value.hashCode())& Integer.MAX_VALUE) % numReduceTasks;
}
```



æ–¹å¼2ï¼š

```java
public class HashPartitioner<K, V> extends Partitioner<K, V> {
private int aa= 0;
    
/** Use {@link Object#hashCode()} to partition. */
public int getPartition(K key, V value,int numReduceTasks) 
{
return (key.hashCode()+(aa++) & Integer.MAX_VALUE) % numReduceTasks;
}
```

> é‡æ–°åˆ†åŒºå™¨



### hadoopæ¡†æ¶ä¸­æ€ä¹ˆæ¥ä¼˜åŒ–

1. ä»åº”ç”¨ç¨‹åºè§’åº¦è¿›è¡Œä¼˜åŒ–ã€‚ç”±äº mapreduceæ˜¯è¿­ä»£é€è¡Œè§£ææ•°æ®æ–‡ä»¶çš„ï¼Œæ€æ ·åœ¨è¿­ä»£çš„æƒ…å†µä¸‹ï¼Œç¼–å†™é«˜æ•ˆç‡çš„åº”ç”¨ç¨‹åºï¼Œæ˜¯ä¸€ç§ä¼˜åŒ–æ€è·¯ã€‚
2.  å¯¹ Hadoopå‚æ•°è¿›è¡Œè°ƒä¼˜ã€‚å½“å‰ hadoopç³»ç»Ÿæœ‰ 190å¤šä¸ªé…ç½®å‚æ•°ï¼Œæ€æ ·è°ƒæ•´è¿™äº›å‚æ•°ï¼Œä½¿ hadoopä½œä¸šè¿è¡Œå°½å¯èƒ½çš„å¿«ï¼Œä¹Ÿæ˜¯ä¸€ç§ä¼˜åŒ–æ€è·¯ã€‚
3. ä»ç³»ç»Ÿå®ç°è§’åº¦è¿›è¡Œä¼˜åŒ–ã€‚è¿™ç§ä¼˜åŒ–éš¾åº¦æ˜¯æœ€å¤§çš„ï¼Œå®ƒæ˜¯ä» hadoopå®ç°æœºåˆ¶è§’åº¦ï¼Œå‘ç°å½“å‰ Hadoopè®¾è®¡å’Œå®ç°ä¸Šçš„ç¼ºç‚¹ï¼Œç„¶åè¿›è¡Œæºç çº§åœ°ä¿®æ”¹ã€‚è¯¥æ–¹æ³•è™½éš¾åº¦å¤§ï¼Œä½†å¾€å¾€æ•ˆæœæ˜æ˜¾ã€‚
4. linuxå†…æ ¸å‚æ•°è°ƒæ•´



### ä»åº”ç”¨ç¨‹åºè§’åº¦è¿›è¡Œä¼˜åŒ–

1. é¿å…ä¸å¿…è¦çš„ reduceä»»åŠ¡

   å¦‚æœmapreduceç¨‹åºä¸­ reduceæ˜¯ä¸å¿…è¦çš„ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥åœ¨ mapä¸­å¤„ç†æ•°æ® , Reducerè®¾ç½®
   ä¸º 0ã€‚è¿™æ · é¿å…äº†å¤šä½™çš„ reduceä»»åŠ¡ã€‚

2. ä¸º jobæ·»åŠ ä¸€ä¸ª Combiner

   ä¸ºjobæ·»åŠ ä¸€ä¸ª combinerå¯ä»¥å¤§å¤§å‡å°‘ shuffleé˜¶æ®µä» map taskæ‹·è´ç»™è¿œç¨‹ reduce taskçš„æ•°æ®é‡ã€‚ä¸€èˆ¬è€Œè¨€ï¼Œ combinerä¸ reducerç›¸åŒã€‚

3. æ ¹æ®å¤„ç†æ•°æ®ç‰¹å¾ä½¿ç”¨æœ€é€‚åˆå’Œç®€æ´çš„ Writableç±»å‹
   Textå¯¹è±¡ä½¿ç”¨èµ·æ¥å¾ˆæ–¹ä¾¿ï¼Œä½†å®ƒåœ¨ç”±æ•°å€¼è½¬æ¢åˆ°æ–‡æœ¬æˆ–æ˜¯ç”± UTF8å­—ç¬¦ä¸²è½¬æ¢åˆ°æ–‡æœ¬æ—¶éƒ½æ˜¯ä½æ•ˆçš„ï¼Œä¸”ä¼šæ¶ˆè€—å¤§é‡çš„ CPUæ—¶é—´ã€‚å½“å¤„ç†é‚£äº›éæ–‡æœ¬çš„æ•°æ®æ—¶ï¼Œå¯ä»¥ä½¿ç”¨äºŒè¿›åˆ¶çš„ Writableç±»å‹ï¼Œå¦‚ IntWritable FloatWritableç­‰ã€‚äºŒè¿›åˆ¶ writableå¥½å¤„ï¼šé¿å…æ–‡ä»¶è½¬æ¢çš„æ¶ˆè€—ï¼›ä½¿map taskä¸­é—´ç»“æœå ç”¨æ›´å°‘çš„ç©ºé—´ã€‚

4. é‡ç”¨ Writableç±»å‹
   å¾ˆå¤šMapReduceç”¨æˆ·å¸¸çŠ¯çš„ä¸€ä¸ªé”™è¯¯æ˜¯ï¼Œåœ¨ä¸€ä¸ª map/reduceæ–¹æ³•ä¸­ä¸ºæ¯ä¸ªè¾“å‡ºéƒ½åˆ›å»ºWritableå¯¹è±¡ã€‚ä¾‹å¦‚ï¼Œä½ çš„ Wordcout mapperæ–¹æ³•å¯èƒ½è¿™æ ·å†™ï¼š

   ```java
   public void map(...) {
   â€¦
   for (String word : words) {
   	output.collect(new Text(word), new IntWritable(1));
   	}
   }
   ```

   è¿™æ ·ä¼šå¯¼è‡´ç¨‹åºåˆ†é…å‡ºæˆåƒä¸Šä¸‡ä¸ªçŸ­å‘¨æœŸçš„å¯¹è±¡ã€‚Javaåƒåœ¾æ”¶é›†å™¨å°±è¦ä¸ºæ­¤åšå¾ˆå¤šçš„å·¥ä½œã€‚åƒåœ¾æ”¶é›†å™¨å°±è¦ä¸ºæ­¤åšå¾ˆå¤šçš„å·¥ä½œã€‚æ›´æœ‰æ•ˆçš„å†™æ³•æ˜¯ï¼šæ›´æœ‰æ•ˆçš„å†™æ³•æ˜¯ï¼š

   æŠ½å–å¯¹è±¡

   ```java
   class MyMapper â€¦ {
   Text wordText = new Text();
       
   IntWritable one = new IntWritable(1);
       
   public void map(...) {
       for (String word: words) {
               wordText.set(word);
               output.collect(wordText, one);
           }
   	}
   }
   ```

   

5.  ä½¿ç”¨ä½¿ç”¨StringBufferè€Œä¸æ˜¯è€Œä¸æ˜¯String

   å½“éœ€è¦å¯¹å­—ç¬¦ä¸²è¿›è¡Œæ“ä½œæ—¶ï¼Œ**ä½¿ç”¨StringBufferè€Œä¸æ˜¯è€Œä¸æ˜¯Stringï¼ŒStringæ˜¯æ˜¯read--onlyçš„ï¼Œå¦‚æœå¯¹çš„ï¼Œå¦‚æœå¯¹å®ƒè¿›è¡Œä¿®æ”¹ï¼Œä¼šäº§ç”Ÿä¸´æ—¶å¯¹è±¡**ï¼Œè€ŒStringBufferæ˜¯å¯ä¿®æ”¹çš„ï¼Œä¸ä¼šäº§ç”Ÿä¸´æ—¶å¯¹è±¡ã€‚æ˜¯å¯ä¿®æ”¹çš„ï¼Œä¸ä¼šäº§ç”Ÿä¸´æ—¶å¯¹è±¡ã€‚
   

### å¯¹å‚æ•°è¿›è¡Œè°ƒä¼˜

æŸ¥çœ‹linuxçš„æœåŠ¡ï¼Œå¯ä»¥å…³é—­ä¸å¿…è¦çš„æœåŠ¡

 ntsysv 



**åœæ­¢æ‰“å°æœåŠ¡**

/etc/init.d/cups stop

chkconfig cups off



**å…³é—­ipv6** 

vim /etc/modprobe.conf 

æ·»åŠ å†…å®¹ 

alias net-pf-10 off alias ipv6 off



**è°ƒæ•´æ–‡ä»¶æœ€å¤§æ‰“å¼€æ•°** 

æŸ¥çœ‹ï¼š ulimit -a  ç»“æœï¼šopen files (-n) 1024 

ä¸´æ—¶ä¿®æ”¹ï¼š ulimit -n 4096 

æŒä¹…ä¿®æ”¹ï¼š

 vi /etc/security/limits.confåœ¨æ–‡ä»¶æœ€ååŠ ä¸Šï¼š

* soft nofile 65535
* hard nofile 65535 
* soft nproc 65535 
* hard nproc 65535



**ä¿®æ”¹linuxå†…æ ¸å‚æ•°**

vi /etc/sysctl.conf 

æ·»åŠ  

net.core.somaxconn = 32768 

webåº”ç”¨ä¸­listenå‡½æ•°çš„backlogé»˜è®¤ä¼šç»™æˆ‘ä»¬å†…æ ¸å‚æ•°çš„net.core.somaxconné™åˆ¶åˆ°128ï¼Œè€Œnginxå®šä¹‰çš„NGX_LISTEN_BACKLOGé»˜è®¤ä¸º511ï¼Œæ‰€ä»¥æœ‰å¿…è¦è°ƒæ•´è¿™ä¸ªå€¼ã€‚ 

è°ƒæ•´swapåˆ†åŒºä»€ä¹ˆæ—¶å€™ä½¿ç”¨ï¼š

æŸ¥çœ‹ï¼šcat /proc/sys/vm/swappiness 

è®¾ç½®ï¼švi /etc/sysctl.conf 

â€‹	åœ¨è¿™ä¸ªæ–‡æ¡£çš„æœ€ååŠ ä¸Šè¿™æ ·ä¸€è¡Œ: vm.swappiness=10 

â€‹	è¡¨ç¤ºç‰©ç†å†…å­˜ä½¿ç”¨åˆ°90%ï¼ˆ100-10=90ï¼‰çš„æ—¶å€™æ‰ä½¿ç”¨swapäº¤æ¢åŒº



**å…³é—­noatime**

vi /etc/fstab /dev/sda2/data ext3	 noatime,nodiratime 0 0



**è®¾ç½®readahead buffer** 

blockdev --setra READAHEAD 512 /dev/sda



**ä»¥ä¸‹æ˜¯ä¿®æ”¹mapred-site.xmlæ–‡ä»¶**

**ä¿®æ”¹æœ€å¤§æ§½ä½æ•°** 

æ§½ä½æ•°æ˜¯åœ¨å„ä¸ªtasktrackerä¸Šçš„mapred-site.xmlä¸Šè®¾ç½®çš„ï¼Œé»˜è®¤éƒ½æ˜¯2

```properties
<property> 
<name>mapred.tasktracker.map.tasks.maximum</name> 
#++++maptaskçš„æœ€å¤§æ•°
<value>2</value>
</property> 

<property> 
<name>mapred.tasktracker.reduce.tasks.maximum</name> 
#++++reducetaskçš„æœ€å¤§æ•° 
<value>2</value> 
</property>
```



**è°ƒæ•´å¿ƒè·³é—´éš”**

é›†ç¾¤è§„æ¨¡å°äº300æ—¶ï¼Œå¿ƒè·³é—´éš”ä¸º300æ¯«ç§’ 

```properties
mapreduce.jobtracker.heartbeat.interval.min å¿ƒè·³æ—¶é—´ 

mapred.heartbeats.in.second é›†ç¾¤æ¯å¢åŠ å¤šå°‘èŠ‚ç‚¹ï¼Œæ—¶é—´å¢åŠ ä¸‹é¢çš„å€¼ 

mapreduce.jobtracker.heartbeat.scaling.factor é›†ç¾¤æ¯å¢åŠ ä¸Šé¢çš„ä¸ªæ•°ï¼Œå¿ƒè·³å¢å¤šå°‘
```



**å¯åŠ¨å¸¦å¤–å¿ƒè·³** 

```properties
mapreduce.tasktracker.outofband.heartbeat é»˜è®¤æ˜¯false
```



**é…ç½®å¤šå—ç£ç›˜** 

```properties
mapreduce.local.dir 
```



**é…ç½®RPC handeræ•°ç›®** 

```properties
mapred.job.tracker.handler.count é»˜è®¤æ˜¯10ï¼Œå¯ä»¥æ”¹æˆ50ï¼Œæ ¹æ®æœºå™¨çš„èƒ½åŠ›
```



**é…ç½®HTTPçº¿ç¨‹æ•°ç›®**

```properties
tasktracker.http.threads é»˜è®¤æ˜¯40ï¼Œå¯ä»¥æ”¹æˆ100 æ ¹æ®æœºå™¨çš„èƒ½åŠ›
```



**é€‰æ‹©åˆé€‚çš„å‹ç¼©æ–¹å¼**

```xml
ä»¥snappyä¸ºä¾‹ï¼š 
<property>
	<name>mapred.compress.map.output</name>
	<value>true</value> 
</property> 

<property>
	<name>mapred.map.output.compression.codec</name>
	<value>org.apache.hadoop.io.compress.SnappyCodec</value>
</property>
```



**å¯ç”¨æ¨æµ‹æ‰§è¡Œæœºåˆ¶**

> â€‹		æ¨æµ‹æ‰§è¡Œ(Speculative Execution)æ˜¯æŒ‡åœ¨åˆ†å¸ƒå¼é›†ç¾¤ç¯å¢ƒä¸‹ï¼Œå› ä¸ºç¨‹åºBUGï¼Œè´Ÿè½½ä¸å‡è¡¡æˆ–è€…èµ„æºåˆ†å¸ƒä¸å‡ç­‰åŸå› ï¼Œé€ æˆåŒä¸€ä¸ªjobçš„å¤šä¸ªtaskè¿è¡Œé€Ÿåº¦ä¸ä¸€è‡´ï¼Œæœ‰çš„taskè¿è¡Œé€Ÿåº¦æ˜æ˜¾æ…¢äºå…¶ä»–taskï¼ˆæ¯”å¦‚ï¼šä¸€ä¸ªjobçš„æŸä¸ªtaskè¿›åº¦åªæœ‰10%ï¼Œè€Œå…¶ä»–æ‰€æœ‰taskå·²ç»è¿è¡Œå®Œæ¯•ï¼‰ï¼Œåˆ™è¿™äº›taskæ‹–æ…¢äº†ä½œä¸šçš„æ•´ä½“æ‰§è¡Œè¿›åº¦ï¼Œä¸ºäº†é¿å…è¿™ç§æƒ…å†µå‘ç”Ÿï¼ŒHadoopä¼šä¸ºè¯¥taskå¯åŠ¨å¤‡ä»½ä»»åŠ¡ï¼Œè®©è¯¥speculative taskä¸åŸå§‹taskåŒæ—¶å¤„ç†ä¸€ä»½æ•°æ®ï¼Œå“ªä¸ªå…ˆè¿è¡Œå®Œï¼Œåˆ™å°†è°çš„ç»“æœä½œä¸ºæœ€ç»ˆç»“æœã€‚ 
>
> â€‹		æ¨æµ‹æ‰§è¡Œä¼˜åŒ–æœºåˆ¶é‡‡ç”¨äº†å…¸å‹çš„ä»¥ç©ºé—´æ¢æ—¶é—´çš„ä¼˜åŒ–ç­–ç•¥ï¼Œå®ƒåŒæ—¶å¯åŠ¨å¤šä¸ªç›¸åŒtaskï¼ˆå¤‡ä»½ä»»åŠ¡ï¼‰å¤„ç†ç›¸åŒçš„æ•°æ®å—ï¼Œå“ªä¸ªå®Œæˆçš„æ—©ï¼Œåˆ™é‡‡ç”¨å“ªä¸ªtaskçš„ç»“æœï¼Œè¿™æ ·å¯é˜²æ­¢æ‹–åè…¿Taskä»»åŠ¡å‡ºç°ï¼Œè¿›è€Œæé«˜ä½œä¸šè®¡ç®—é€Ÿåº¦ï¼Œä½†æ˜¯ï¼Œè¿™æ ·å´ä¼šå ç”¨æ›´å¤šçš„èµ„æºï¼Œåœ¨é›†ç¾¤èµ„æºç´§ç¼ºçš„æƒ…å†µä¸‹ï¼Œè®¾è®¡åˆç†çš„æ¨æµ‹æ‰§è¡Œæœºåˆ¶å¯åœ¨å¤šç”¨å°‘é‡èµ„æºæƒ…å†µä¸‹ï¼Œå‡å°‘å¤§ä½œä¸šçš„è®¡ç®—æ—¶é—´ã€‚

```properties
mapred.map.tasks.speculative.execution é»˜è®¤æ˜¯true

mapred.rduce.tasks.speculative.execution é»˜è®¤æ˜¯true
```



**è®¾ç½®å¤±è´¥å®¹å¿åº¦**

```properties
mapred.max.map.failures.percent ä½œä¸šå…è®¸å¤±è´¥çš„mapæœ€å¤§æ¯”ä¾‹ é»˜è®¤å€¼0ï¼Œå³0%

mapred.max.reduce.failures.percent ä½œä¸šå…è®¸å¤±è´¥çš„reduceæœ€å¤§æ¯”ä¾‹ é»˜è®¤å€¼0ï¼Œå³0%

mapred.map.max.attemps å¤±è´¥åæœ€å¤šé‡æ–°å°è¯•çš„æ¬¡æ•° é»˜è®¤æ˜¯4 

mapred.reduce.max.attemps å¤±è´¥åæœ€å¤šé‡æ–°å°è¯•çš„æ¬¡æ•° é»˜è®¤æ˜¯4
```



**å¯åŠ¨jvmé‡ç”¨åŠŸèƒ½**

```properties
mapred.job.reuse.jvm.num.tasks é»˜è®¤å€¼1ï¼Œè¡¨ç¤ºåªèƒ½å¯åŠ¨ä¸€ä¸ªtaskï¼Œè‹¥ä¸º-1ï¼Œè¡¨ç¤ºå¯ä»¥æœ€å¤šè¿è¡Œæ•°ä¸é™åˆ¶
```



**è®¾ç½®ä»»åŠ¡è¶…æ—¶æ—¶é—´**

```properties
mapred.task.timeout é»˜è®¤å€¼600000æ¯«ç§’ï¼Œä¹Ÿå°±æ˜¯10åˆ†é’Ÿã€‚
```



**åˆç†çš„æ§åˆ¶reduceçš„å¯åŠ¨æ—¶é—´**

```properties
mapred.reduce.slowstart.completed.maps é»˜è®¤å€¼0.05 è¡¨ç¤ºmapä»»åŠ¡å®Œæˆ5%æ—¶ï¼Œå¼€å§‹å¯åŠ¨reduceä»»åŠ¡
```



**è·³è¿‡åè®°å½•** 

å½“ä»»åŠ¡å¤±è´¥æ¬¡æ•°è¾¾åˆ°è¯¥å€¼æ—¶ï¼Œæ‰ä¼šè¿›å…¥skip modeï¼Œå³å¯ç”¨è·³è¿‡åè®°å½•æ•°åŠŸèƒ½,ä¹Ÿå°±æ˜¯å…ˆè¯•å‡ æ¬¡ï¼Œä¸è¡Œå°±è·³è¿‡

mapred.skip.attempts.to.start.skipping é»˜è®¤å€¼ 2 



```properties
mapæœ€å¤šå…è®¸è·³è¿‡çš„è®°å½•æ•° 

mapred.skip.map.max.skip.records é»˜è®¤å€¼0ï¼Œä¸ºä¸å¯ç”¨ 

reduceæœ€å¤šå…è®¸è·³è¿‡çš„è®°å½•æ•° 
mapred.skip.reduce.max.skip.records é»˜è®¤å€¼0ï¼Œä¸ºä¸å¯ç”¨

æ¢è®°å½•å­˜æ”¾çš„ç›®å½•
mapred.skip.out.dir é»˜è®¤å€¼${mapred.output.dir}/_logs/
```



### æˆ‘ä»¬å¼€å‘ job æ—¶ï¼Œæ˜¯å¦å¯ä»¥å»æ‰ reduce é˜¶æ®µ

å¯ä»¥ã€‚è®¾ç½® reduce æ•°ä¸º 0 å³å¯ã€‚



### datanode åœ¨ä»€ä¹ˆæƒ…å†µä¸‹ä¸ä¼šå¤‡ä»½ 

datanode åœ¨å¼ºåˆ¶å…³é—­æˆ–è€…éæ­£å¸¸æ–­ç”µä¸ä¼šå¤‡ä»½ã€‚



### combiner å‡ºç°åœ¨é‚£ä¸ªè¿‡ç¨‹

å‡ºç°åœ¨ map é˜¶æ®µçš„ map æ–¹æ³•åã€‚



### ğŸ˜ºhdfs çš„ä½“ç³»ç»“æ„ 

hdfs æœ‰ namenodeã€secondraynamenodeã€datanode ç»„æˆã€‚ 

ä¸º n+1 æ¨¡å¼ 

namenode è´Ÿè´£ç®¡ç† datanode å’Œè®°å½•å…ƒæ•°æ® 

secondraynamenode è´Ÿè´£åˆå¹¶æ—¥å¿— 

datanode è´Ÿè´£å­˜å‚¨æ•°æ®



### 3 ä¸ª datanode ä¸­æœ‰ä¸€ä¸ª datanode å‡ºç°é”™è¯¯ä¼šæ€æ ·ï¼Ÿ 

è¿™ä¸ª datanode çš„æ•°æ®ä¼šåœ¨å…¶ä»–çš„ datanode ä¸Šé‡æ–°åšå¤‡ä»½ã€‚



### æè¿°ä¸€ä¸‹ hadoop ä¸­ï¼Œæœ‰å“ªäº›åœ°æ–¹ä½¿ç”¨äº†ç¼“å­˜æœºåˆ¶ï¼Œ ä½œç”¨åˆ†åˆ«æ˜¯ä»€ä¹ˆï¼Ÿ

åœ¨ mapreduce æäº¤ job çš„è·å– id ä¹‹åï¼Œä¼šå°†æ‰€æœ‰æ–‡ä»¶å­˜å‚¨åˆ°åˆ†å¸ƒå¼ç¼“å­˜ä¸Šï¼Œè¿™æ ·æ–‡ä»¶å¯ä»¥ è¢«æ‰€æœ‰çš„ mapreduce å…±äº«ã€‚



### å¦‚ä½•ç¡®å®š hadoop é›†ç¾¤çš„å¥åº·çŠ¶æ€ 

é€šè¿‡é¡µé¢ç›‘æ§,è„šæœ¬ç›‘æ§ã€‚



### ç”Ÿäº§ç¯å¢ƒä¸­ä¸ºä»€ä¹ˆå»ºè®®ä½¿ç”¨å¤–éƒ¨è¡¨ï¼Ÿ 

1ã€å› ä¸ºå¤–éƒ¨è¡¨ä¸ä¼šåŠ è½½æ•°æ®åˆ° hiveï¼Œå‡å°‘æ•°æ®ä¼ è¾“ã€æ•°æ®è¿˜èƒ½å…±äº«ã€‚ 

2ã€hive ä¸ä¼šä¿®æ”¹æ•°æ®ï¼Œæ‰€ä»¥æ— éœ€æ‹…å¿ƒæ•°æ®çš„æŸå 

3ã€ åˆ é™¤è¡¨æ—¶ï¼Œåªåˆ é™¤è¡¨ç»“æ„ã€ä¸åˆ é™¤æ•°æ®ã€‚



## é¢è¯•çœŸé¢˜

### è¯·é€‰æ‹©ä½ ç†Ÿç»ƒæŒæ¡çš„hadoopç‰ˆæœ¬ï¼Œå¹¶åŸºäºæ­¤å›ç­”ä¸‹åˆ—é—®é¢˜

hadoop1.0	

#### hadoopçš„æ ¸å¿ƒé…ç½®æ–‡ä»¶åç§°æ˜¯ä»€ä¹ˆï¼Ÿ

core-site.xml

#### "jps"å‘½ä»¤çš„ç”¨å¤„ï¼Ÿ

æŸ¥çœ‹hadoopèŠ‚ç‚¹è¿›ç¨‹

#### å¦‚ä½•æ£€æŸ¥namenodeæ˜¯å¦æ­£å¸¸è¿è¡Œï¼Ÿé‡å¯namenodeçš„å‘½ä»¤æ˜¯ä»€ä¹ˆï¼Ÿ

é€šè¿‡èŠ‚ç‚¹ä¿¡æ¯å’Œæµè§ˆå™¨æŸ¥çœ‹ï¼Œé€šè¿‡è„šæœ¬ç›‘æ§
hadoop deamon.sh start namenode
hdfs deamon.sh start namenode

#### é¿å…namenodeæ•…éšœå¯¼è‡´é›†ç¾¤å®•æœºçš„è§£å†³æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ

è‡ªå·±ä¹¦å†™è„šæœ¬ç›‘æ§é‡å¯

#### hbaseæ•°æ®åº“å¯¹è¡Œé”®çš„è®¾è®¡è¦æ±‚æ˜¯ä»€ä¹ˆï¼Ÿ

è¡Œå¥ä»¥å­—å…¸åºæ’åˆ—ï¼Œè®¾è®¡æ—¶å……åˆ†åˆ©ç”¨è¿™ä¸ªç‰¹ç‚¹ï¼Œå°†ç»å¸¸ä¸€èµ·æŸ¥è¯¢çš„è¡Œå¥è®¾è®¡åœ¨ä¸€èµ·ï¼Œä¾‹
å¦‚æ—¶é—´æˆ³ç»“å°¾ï¼Œç”¨æˆ·åå¼€å¤´ï¼ˆä½ç½®ç›¸å…³æ€§ï¼‰



### Hadoopé¢è¯•ä¸šåŠ¡é¢˜

**ä¸šåŠ¡åœºæ™¯ï¼š**

#### ç”¨æˆ·è®¿é—®ç½‘ç«™æ—¶ï¼Œæ¯ä¸ªé¡µé¢ä¼šä¸ŠæŠ¥ä¸€æ¡pvæ•°æ®ï¼ŒåŒæ—¶åšä¸€äº›ä¸šåŠ¡æ“ä½œï¼Œä¼šä¸ŠæŠ¥äº‹ä»¶æ•°æ®ï¼Œå¦‚ï¼š

1. ç”¨æˆ·æµè§ˆé¡µé¢ï¼ˆPVï¼‰
2. ç”¨æˆ·äº‹ä»¶è¡Œä¸ºï¼ˆå¼€æˆ·ï¼Œä¸‹å•ä¹°åŸºé‡‘......ï¼‰
3. é¡µé¢clickç‚¹å‡»ï¼ˆåŒ…å«å„ç§è¶…é“¾æ¥ï¼Œå¯ç‚¹å‡»æŒ‰é’®ï¼Œradioï¼Œcheckbox......ï¼‰

æ¯æ—¥ï¼Œä»¥ä¸Š3é¡¹çš„ä¸ŠæŠ¥æ•°å¤§è‡´10000000



**ä¸šåŠ¡éœ€æ±‚ï¼š**

1. éœ€è¦æŒ‰æ—¶é—´ç»´åº¦ï¼ˆå¤©ï¼Œå‘¨ï¼‰ï¼ŒæŸç§ä¸šåŠ¡ç»´åº¦ï¼ˆå¼€æˆ·ï¼Œä¹°åŸºé‡‘...ï¼‰ï¼Œå®šæ—¶åšç»Ÿè®¡ï¼ˆæ€»äººæ•°ï¼Œé‡‘é¢ç­‰ï¼‰ã€‚

ä¾‹ï¼šè¿‡å»ä¸€å‘¨ï¼ˆéš”æ—¥ï¼‰çš„pvï¼Œuvæ•°ï¼Œäº¤æ˜“æ€»é‡‘é¢ã€‚



2. éœ€è¦å›æº¯å†å²æ•°æ®ï¼Œå¦‚ï¼šè¿‡å»æŸä¸ªæ—¶é—´ç‚¹ï¼ˆæ®µï¼‰ï¼Œè®¿é—®è¿‡æŸé¡µé¢çš„ç”¨æˆ·ï¼Œåœ¨æŸä¸ªæ—¶é—´ç‚¹ï¼ˆæ®µï¼‰å¯¼è‡´æŸç§ä¸šåŠ¡å‘ç”Ÿçš„ç»Ÿè®¡æ•°æ®ã€‚

ä¾‹ï¼šåœ¨2015-01-01è‡³2015-01-31è®¿é—®Aé¡µé¢ï¼Œå¹¶åœ¨2015-01-01è‡³2015-03-31å¼€æˆ·ï¼Œä¸‹å•çš„ç”¨æˆ·æ•°



**æŠ€æœ¯æ–¹æ¡ˆï¼š**

è¯·ç»™å‡ºä½ çš„è®¾è®¡æ–¹æ¡ˆï¼Œæ¯”å¦‚ä½¿ç”¨å“ªäº›æŠ€æœ¯æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èµ·åˆ°çš„ä½œç”¨ç­‰ã€‚

1. ç”¨hiveåˆ†æä¸šåŠ¡æ•°æ®å³å¯

2. è®²æ•°æ®å¯¼å…¥åˆ°hiveä¸­

   sqlçš„è®¾è®¡æ€è·¯ï¼šå¤šè¡¨å…³è”

   1. æ‰¾åˆ°æ‰€æœ‰åœ¨ 2015 01 01åˆ° 2015 01 31æ—¶é—´å†…è®¿é—® Aé¡µé¢çš„ç”¨æˆ·
   2. åœ¨è¿™äº›ç”¨æˆ·ä¸­ç­›é€‰åœ¨ 2015 01 01åˆ° 2015 03 31ä¸‹å•çš„ç”¨æˆ·
   3. ç»Ÿè®¡æ€»æ•°



### ä½ ä»¬æ•°æ®åº“æ€ä¹ˆå¯¼å…¥ hive çš„ ,æœ‰æ²¡æœ‰å‡ºç°é—®é¢˜

åœ¨å¯¼å…¥hiveçš„æ—¶å€™ï¼Œå¦‚æœæ•°æ®åº“ä¸­æœ‰ blobæˆ–è€… textå­—æ®µï¼Œä¼šæŠ¥é”™ï¼Œè§£å†³æ–¹æ¡ˆåœ¨ sqoopç¬”è®°ä¸­ã€‚



### å…¬å¸æŠ€æœ¯é€‰å‹å¯èƒ½åˆ©ç”¨ storm è¿›è¡Œå®æ—¶è®¡ç®— ,è®²è§£ä¸€ä¸‹storm

æè¿°ä¸‹stormçš„è®¾è®¡æ¨¡å¼ï¼Œæ˜¯åŸºäº workã€ excutorã€ taskçš„æ–¹å¼è¿è¡Œä»£ç ï¼Œç”± spoutã€ boltç»„
æˆç­‰ç­‰



### ä¸€ä¸ª datanode å®•æœº ,æ€ä¹ˆä¸€ä¸ªæµç¨‹æ¢å¤

å°†datanodeæ•°æ®åˆ é™¤ï¼Œé‡æ–°å½“æˆæ–°èŠ‚ç‚¹åŠ å…¥å³å¯ã€‚



### Hbase çš„ç‰¹æ€§ ,ä»¥åŠä½ æ€ä¹ˆå»è®¾è®¡ rowkey å’Œ columnFamily ,æ€ä¹ˆå»å»ºä¸€ä¸ª table

hbaseæ˜¯åˆ—å¼æ•°æ®åº“ï¼Œ rowkeyæ˜¯å­—å…¸åºçš„ï¼Œè®¾è®¡æ—¶çš„è§„åˆ™åŒä¸Šã€‚
æ¯ä¸ªåˆ—æ—æ˜¯ä¸€ä¸ªæ–‡ä»¶ï¼Œå°†ç»å¸¸ä¸€èµ·æŸ¥è¯¢çš„åˆ—æ”¾åˆ°åŒä¸€ä¸ªåˆ—æ—ä¸­ï¼Œå‡å°‘æ–‡ä»¶çš„å¯»å€æ—¶é—´ã€‚



### â¤ï¸Redis,ä¼ ç»Ÿæ•°æ®åº“ ,hbase,hive æ¯ä¸ªä¹‹é—´çš„åŒºåˆ«

- redisï¼šåˆ†å¸ƒå¼ç¼“å­˜ï¼Œå¼ºè°ƒç¼“å­˜ï¼Œå†…å­˜ä¸­æ•°æ®
- ä¼ ç»Ÿæ•°æ®åº“ï¼šæ³¨é‡å…³ç³»
- hbaseï¼šåˆ—å¼æ•°æ®åº“ï¼Œæ— æ³•åšå…³ç³»æ•°æ®åº“çš„ä¸»å¤–é”®ï¼Œç”¨äºå­˜å‚¨æµ·é‡æ•°æ®ï¼Œåº•å±‚åŸºäº hdfs
- hiveï¼šæ•°æ®ä»“åº“å·¥å…·ï¼Œåº•å±‚æ˜¯ mapreduceã€‚ä¸æ˜¯æ•°æ®åº“ï¼Œä¸èƒ½ç”¨æ¥åšç”¨æˆ·çš„äº¤äº’å­˜å‚¨



### â¤ï¸shuffle é˜¶æ®µ ,ä½ æ€ä¹ˆç†è§£çš„

shuffleçš„è¿‡ç¨‹è¯´æ¸…æ¥šï¼Œç›®çš„è¯´æ¸…æ¥š



### Mapreduce çš„ map æ•°é‡ å’Œ reduce æ•°é‡ æ€ä¹ˆç¡®å®š ,æ€ä¹ˆé…ç½®

mapçš„æ•°é‡æœ‰æ•°æ®å—å†³å®šï¼Œ reduceæ•°é‡éšä¾¿é…ç½®ã€‚



### stormå®æ—¶è®¡ç®—

å”¯ä¸€éš¾ä½æˆ‘çš„æ˜¯ä»–è¯´å®æ—¶è®¡ç®— ,storm å¦‚æœç¢°ä¸Šäº†å¤æ‚é€»è¾‘ ,éœ€è¦ç®—å¾ˆé•¿çš„æ—¶é—´ ,ä½ æ€ä¹ˆå»ä¼˜åŒ– ,æ€ä¹ˆä¿è¯å®æ—¶æ€§



### Hive ä½ ä»¬ç”¨çš„æ˜¯å¤–éƒ¨è¡¨è¿˜æ˜¯å†…éƒ¨è¡¨ ,æœ‰æ²¡æœ‰å†™è¿‡UDF,hive çš„ç‰ˆæœ¬

å¤–éƒ¨è¡¨å’Œå†…éƒ¨è¡¨çš„åŒºåˆ«



### Hadoop çš„ç‰ˆæœ¬

1.04ã€ 1.20éƒ½ä¸ºç¨³å®šç‰ˆï¼Œæ˜¯ä¸¤ä¸ªå¸¸ç”¨çš„ hadoop1ç‰ˆæœ¬ã€‚



### å®æ—¶æµå¼è®¡ç®—çš„ç»“æœå†…å®¹æœ‰å“ªäº› ,ä½ ä»¬éœ€è¦ç»Ÿè®¡å‡ºæ¥ä¹ˆ



### è®¾è®¡æ—¥å¿—æ”¶é›†åˆ†æç³»ç»Ÿ

æ—¥å¿—åˆ†å¸ƒåœ¨å„ä¸ªä¸šåŠ¡ç³»ç»Ÿä¸­ã€‚æˆ‘ä»¬éœ€è¦å¯¹å½“å¤©çš„æ—¥å¿—è¿›è¡Œå®æ—¶æ±‡æ€»ç»Ÿè®¡ï¼ŒåŒæ—¶åˆèƒ½æŸ¥è¯¢å†å²çš„æ±‡æ€»æ•°æ®ï¼ˆå¯ä»¥å›´ç»•PVã€UVã€IPç­‰æŒ‡æ ‡è¿›è¡Œé˜è¿°ï¼‰

1. é€šè¿‡ flumeå°†ä¸åŒç³»ç»Ÿçš„æ—¥å¿—æ”¶é›†åˆ° kafkaä¸­
2. é€šè¿‡ stormå®æ—¶çš„å¤„ç† PVã€ UVã€ IP
3. é€šè¿‡ kafkaçš„ consumerå°†æ—¥å¿—ç”Ÿäº§åˆ° hbaseä¸­ã€‚
4. é€šè¿‡ç¦»çº¿çš„ mapreduceæˆ–è€… hiveï¼Œå¤„ç† hbaseä¸­çš„æ•°æ®




### å¦‚æœä½ æ¥åšæŠ€æœ¯åˆ†äº«ï¼Œä½ ä¼šé€‰æ‹©ä»€ä¹ˆä¸»é¢˜ï¼Œè¯¾ç¨‹å®‰æ’æ—¶æ€ä¹ˆæ ·çš„ï¼Ÿ

å¤§ä½“åˆ†ä¸º3ä¸ªéƒ¨åˆ† :
1ã€ ç¦»çº¿ hadoopæŠ€æœ¯åˆ†äº«ï¼ˆ mapreduceã€ hiveï¼‰
2ã€ nosqlæ•°æ®åº“hbaseåˆ†äº«
3ã€ å®æ—¶æµè®¡ç®—åˆ†äº«



### Hiveè¯­å¥å®ç°WordCount

å‡è®¾æ•°æ®å­˜å‚¨åœ¨hadoopä¸‹ï¼Œè·¯å¾„ä¸ºï¼š/home/hadoop/wordcounté‡Œé¢å…¨æ˜¯ä¸€äº›å•è¯

1. å»ºè¡¨
2. åˆ†ç»„ï¼ˆgroup byï¼‰ç»Ÿè®¡wordcount

```sql
select word,count(1) from table1 group by word;
```



### å¤§æ•°æ®ç›¸åŒå­—ç¬¦æ¯”å¯¹

ç»™å®šaï¼Œbä¸¤ä¸ªæ–‡ä»¶ï¼Œå„å­˜æ”¾50äº¿ä¸ªurlï¼Œæ¯ä¸ªurlå„å 64å­—èŠ‚ï¼Œå†…å­˜é™åˆ¶æ˜¯4Gï¼Œæ‰¾å‡ºaã€bæ–‡ä»¶å…±åŒçš„urlï¼Ÿ

å¯ä»¥ä¼°è®¡æ¯ä¸ªæ–‡ä»¶çš„å¤§å°ä¸º50G*64=298Gï¼Œè¿œè¿œå¤§äºå†…å­˜é™åˆ¶çš„4G.æ‰€ä»¥ä¸å¯èƒ½å°†å…¶å®Œå…¨åŠ è½½åˆ°å†…å­˜ä¸­å¤„ç†ã€‚è€ƒè™‘é‡‡å–**åˆ†è€Œæ²»ä¹‹**çš„æ–¹æ³•ã€‚

1. å°†æ–‡ä»¶å­˜å‚¨åˆ° hdfsä¸­ï¼Œè¿™æ ·æ¯ä¸ªæ–‡ä»¶ä¸º 64Mæˆ–è€…æ˜¯ 128M
2. åˆ†åˆ«å¯¹ä¸¤ä¸ªæ–‡ä»¶çš„ urlè¿›è¡Œå»é‡ã€æ’åºè¾“å‡ºï¼Œè¿™æ ·èƒ½æ’é™¤aæ–‡ä»¶ä¸­ç›¸åŒçš„urlï¼Œbæ–‡ä»¶ä¹Ÿä¸€æ ·
3. å¯¹ aã€bä¸¤ä¸ªæ–‡ä»¶å¤„ç†åçš„ç»“æœè¿›è¡Œ wordcountï¼Œå¹¶ä¸”åœ¨ reduceä¸­åˆ¤æ–­å•è¯ä¸ªæ•°ï¼Œä¸ªæ•°ä¸º 2çš„æ—¶å€™è¾“å‡ºï¼Œè¿™æ ·å°±æ‰¾åˆ°äº† aã€ bæ–‡ä»¶ä¸­çš„ç›¸åŒ urlã€‚
4. æ­¤è®¡ç®—æ­¥éª¤ä¸­çš„æ¯ä¸€æ­¥åŠ è½½åˆ°å†…å­˜ä¸­çš„æ–‡ä»¶å¤§å°éƒ½ä¸ä¼šè¶…è¿‡ 64Mï¼Œè¿œè¿œå°äº 4Gã€‚



### ğŸ˜¶ä¸€äº¿ä¸ªæ•°æ®è·å–å‰100ä¸ªæœ€å¤§å€¼ï¼ˆæ­¥éª¤åŠç®—æ³•å¤æ‚åº¦ï¼‰

topkï¼Œå¼ºè°ƒä½¿ç”¨treemapæ˜¯ä¸ºäº†èŠ‚çœè®¡ç®—ç©ºé—´



### å®æ—¶æ•°æ®ç»Ÿè®¡ä¼šç”¨åˆ°å“ªäº›æŠ€æœ¯ï¼Œä»–ä»¬å„è‡ªçš„åº”ç”¨åœºæ™¯åŠåŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ

flumeï¼šæ—¥å¿—æ”¶é›†ç³»ç»Ÿï¼Œä¸»è¦ç”¨äºç³»ç»Ÿæ—¥å¿—çš„æ”¶é›†
kafkaï¼šæ¶ˆæ¯é˜Ÿåˆ—ï¼Œè¿›è¡Œæ¶ˆæ¯çš„ç¼“å­˜å’Œç³»ç»Ÿçš„è§£è€¦
stormï¼šå®æ—¶è®¡ç®—æ¡†æ¶ï¼Œè¿›è¡Œæµå¼çš„è®¡ç®—



### Stringå’ŒStringBufferçš„åŒºåˆ«ï¼ŒStringBufferå’ŒStringBuilderçš„åŒºåˆ«

ç®€å•åœ°è¯´ï¼Œå°±æ˜¯ä¸€ä¸ªå˜é‡å’Œå¸¸é‡çš„å…³ç³»ã€‚StringBufferå¯¹è±¡çš„å†…å®¹å¯ä»¥ä¿®æ”¹ï¼›è€Œ Stringå¯¹è±¡ä¸€æ—¦äº§ç”Ÿåå°±ä¸å¯ä»¥è¢«ä¿®æ”¹ï¼Œé‡æ–°èµ‹å€¼å…¶å®æ˜¯ä¸¤ä¸ªå¯¹è±¡ã€‚

StringBuilderï¼šçº¿ç¨‹éå®‰å…¨çš„
StringBufferï¼šçº¿ç¨‹å®‰å…¨çš„


å½“æˆ‘ä»¬åœ¨å­—ç¬¦ä¸²ç¼“å†²åŒºè¢«å¤šä¸ªçº¿ç¨‹ä½¿ç”¨æ—¶ï¼Œ JVMä¸èƒ½ä¿è¯ StringBuilderçš„æ“ä½œæ˜¯å®‰å…¨çš„ï¼Œè™½ç„¶ä»–çš„é€Ÿåº¦æœ€å¿«ã€‚ä½†æ˜¯å¯ä»¥ä¿è¯ StringBufferæ˜¯å¯ä»¥æ­£ç¡®æ“ä½œçš„ã€‚

å½“ç„¶å¤§å¤šæ•°æƒ…å†µä¸‹å°±æ˜¯æˆ‘ä»¬æ˜¯åœ¨å•çº¿ç¨‹ä¸‹è¿›è¡Œçš„æ“ä½œï¼Œæ‰€ä»¥å¤§å¤šæ•°æƒ…å†µä¸‹æ˜¯å»ºè®®ç”¨ StringBuilderè€Œä¸ç”¨StringBufferçš„ï¼Œå°±æ˜¯é€Ÿåº¦çš„åŸå› ã€‚



## 

### â¤ï¸HashTableå’ŒHashMapã€ArrayListå’ŒVectorã€ArrayListå’ŒLinkedListçš„åŒºåˆ«

#### HashMapä¸æ˜¯çº¿ç¨‹å®‰å…¨çš„

hastmapæ˜¯ä¸€ä¸ªæ¥å£æ˜¯mapæ¥å£çš„å­æ¥å£ï¼Œæ˜¯å°†é”®æ˜ å°„åˆ°å€¼çš„å¯¹è±¡ï¼Œå…¶ä¸­é”®å’Œå€¼éƒ½æ˜¯å¯¹è±¡ï¼Œå¹¶ä¸”ä¸èƒ½åŒ…å«é‡å¤é”®ï¼Œä½†å¯ä»¥åŒ…å«é‡å¤å€¼ã€‚HashMapå…è®¸null keyå’Œnull valueï¼Œè€Œhashtableä¸å…è®¸ã€‚

HashTableæ˜¯çº¿ç¨‹å®‰å…¨çš„ä¸€ä¸ªCollection

HashMapæ˜¯Hashtableçš„è½»é‡çº§å®ç°ï¼ˆéçº¿ç¨‹å®‰å…¨çš„å®ç°ï¼‰ï¼Œä»–ä»¬éƒ½å®Œæˆäº†Mapæ¥å£ï¼Œä¸»è¦åŒºåˆ«åœ¨äºHashMapå…è®¸ç©ºï¼ˆnullï¼‰é”®å€¼ï¼ˆkeyï¼‰,ç”±äºéçº¿ç¨‹å®‰å…¨ï¼Œæ•ˆç‡ä¸Šå¯èƒ½é«˜äºHashtableã€‚ HashMapå…è®¸å°†nullä½œä¸ºä¸€ä¸ªentryçš„keyæˆ–è€…valueï¼Œè€ŒHashtableä¸å…è®¸ã€‚ 

HashMapæŠŠHashtableçš„containsæ–¹æ³•å»æ‰äº†ï¼Œæ”¹æˆcontainsvalueå’ŒcontainsKeyã€‚å› ä¸ºcontainsæ–¹æ³•å®¹æ˜“è®©äººå¼•èµ·è¯¯è§£ã€‚ 

Hashtableç»§æ‰¿è‡ªDictionaryç±»ï¼Œè€ŒHashMapæ˜¯Java1.2å¼•è¿›çš„Map interfaceçš„ä¸€ä¸ªå®ç°ã€‚ æœ€å¤§çš„ä¸åŒæ˜¯ï¼ŒHashtableçš„æ–¹æ³•æ˜¯Synchronizeçš„ï¼Œè€ŒHashMapä¸æ˜¯ï¼Œåœ¨å¤šä¸ªçº¿ç¨‹è®¿é—®Hashtableæ—¶ï¼Œä¸éœ€è¦è‡ªå·±ä¸ºå®ƒçš„æ–¹æ³•å®ç°åŒæ­¥ï¼Œè€ŒHashMap å°±å¿…é¡»ä¸ºä¹‹æä¾›å¤–åŒæ­¥ã€‚ Hashtableå’ŒHashMapé‡‡ç”¨çš„hash/rehashç®—æ³•éƒ½å¤§æ¦‚ä¸€æ ·ï¼Œæ‰€ä»¥æ€§èƒ½ä¸ä¼šæœ‰å¾ˆå¤§çš„å·®ã€‚

#### HashTableå¯¹æ¯”HashMap

| hashmap   | çº¿ç¨‹ä¸å®‰å…¨ | å…è®¸æœ‰nullçš„é”®å’Œå€¼   | æ•ˆç‡é«˜ä¸€ç‚¹ | æ–¹æ³•ä¸æ˜¯Synchronizeçš„è¦æä¾›å¤–åŒæ­¥ | æœ‰containsvalueå’ŒcontainsKeyæ–¹æ³• |
| --------- | ---------- | -------------------- | ---------- | --------------------------------- | -------------------------------- |
| hashtable | çº¿ç¨‹å®‰å…¨   | ä¸å…è®¸æœ‰nullçš„é”®å’Œå€¼ | æ•ˆç‡ç¨ä½   | æ–¹æ³•æ˜¯æ˜¯Synchronizeçš„             | æœ‰containsæ–¹æ³•                   |

#### Hashtableå’ŒHashMapç±»æœ‰ä¸‰ä¸ªé‡è¦çš„ä¸åŒä¹‹å¤„ã€‚

ç¬¬ä¸€ä¸ªä¸åŒä¸»è¦æ˜¯å†å²åŸå› ã€‚Hashtableæ˜¯åŸºäºé™ˆæ—§çš„Dictionaryç±»çš„ï¼ŒHashMapæ˜¯Java 1.2å¼•è¿›çš„Mapæ¥å£çš„ä¸€ä¸ªå®ç°ã€‚

ä¹Ÿè®¸æœ€é‡è¦çš„ä¸åŒæ˜¯Hashtableçš„æ–¹æ³•æ˜¯åŒæ­¥çš„ï¼Œè€ŒHashMapçš„æ–¹æ³•ä¸æ˜¯ã€‚è¿™å°±æ„å‘³ç€ï¼Œè™½ç„¶ä½ å¯ä»¥ä¸ç”¨é‡‡å–ä»»ä½•ç‰¹æ®Šçš„è¡Œä¸ºå°±å¯ä»¥åœ¨ä¸€ä¸ªå¤šçº¿ç¨‹çš„åº”ç”¨ç¨‹åºä¸­ç”¨ä¸€ä¸ªHashtableï¼Œä½†ä½ å¿…é¡»åŒæ ·åœ°ä¸ºä¸€ä¸ªHashMapæä¾›å¤–åŒæ­¥ã€‚ä¸€ä¸ªæ–¹ä¾¿çš„æ–¹æ³•å°±æ˜¯åˆ©ç”¨Collectionsç±»çš„é™æ€çš„synchronizedMap()æ–¹æ³•ï¼Œå®ƒåˆ›å»ºä¸€ä¸ªçº¿ç¨‹å®‰å…¨çš„Mapå¯¹è±¡ï¼Œå¹¶æŠŠå®ƒä½œä¸ºä¸€ä¸ªå°è£…çš„å¯¹è±¡æ¥è¿”å›ã€‚è¿™ä¸ªå¯¹è±¡çš„æ–¹æ³•å¯ä»¥è®©ä½ åŒæ­¥è®¿é—®æ½œåœ¨çš„HashMapã€‚è¿™ä¹ˆåšçš„ç»“æœå°±æ˜¯å½“ä½ ä¸éœ€è¦åŒæ­¥æ—¶ï¼Œä½ ä¸èƒ½åˆ‡æ–­Hashtableä¸­çš„åŒæ­¥ï¼ˆæ¯”å¦‚åœ¨ä¸€ä¸ªå•çº¿ç¨‹çš„åº”ç”¨ç¨‹åºä¸­ï¼‰ï¼Œè€Œä¸”åŒæ­¥å¢åŠ äº†å¾ˆå¤šå¤„ç†è´¹ç”¨ã€‚

ç¬¬ä¸‰ç‚¹ä¸åŒæ˜¯ï¼Œåªæœ‰HashMapå¯ä»¥è®©ä½ å°†ç©ºå€¼ä½œä¸ºä¸€ä¸ªè¡¨çš„æ¡ç›®çš„keyæˆ–valueã€‚HashMapä¸­åªæœ‰ä¸€æ¡è®°å½•å¯ä»¥æ˜¯ä¸€ä¸ªç©ºçš„keyï¼Œä½†ä»»æ„æ•°é‡çš„æ¡ç›®å¯ä»¥æ˜¯ç©ºçš„valueã€‚è¿™å°±æ˜¯è¯´ï¼Œå¦‚æœåœ¨è¡¨ä¸­æ²¡æœ‰å‘ç°æœç´¢é”®ï¼Œæˆ–è€…å¦‚æœå‘ç°äº†æœç´¢é”®ï¼Œä½†å®ƒæ˜¯ä¸€ä¸ªç©ºçš„å€¼ï¼Œé‚£ä¹ˆget()å°†è¿”å›nullã€‚å¦‚æœæœ‰å¿…è¦ï¼Œç”¨containKey()æ–¹æ³•æ¥åŒºåˆ«è¿™ä¸¤ç§æƒ…å†µã€‚

ä¸€äº›èµ„æ–™å»ºè®®ï¼Œå½“éœ€è¦åŒæ­¥æ—¶ï¼Œç”¨Hashtableï¼Œåä¹‹ç”¨HashMapã€‚ä½†æ˜¯ï¼Œå› ä¸ºåœ¨éœ€è¦æ—¶ï¼ŒHashMapå¯ä»¥è¢«åŒæ­¥ï¼ŒHashMapçš„åŠŸèƒ½æ¯”Hashtableçš„åŠŸèƒ½æ›´å¤šï¼Œè€Œä¸”å®ƒä¸æ˜¯åŸºäºä¸€ä¸ªé™ˆæ—§çš„ç±»çš„ï¼Œæ‰€ä»¥æœ‰äººè®¤ä¸ºï¼Œåœ¨å„ç§æƒ…å†µä¸‹ï¼ŒHashMapéƒ½ä¼˜å…ˆäºHashtableã€‚

#### Vector & ArrayList

Vectorçš„æ–¹æ³•éƒ½æ˜¯åŒæ­¥çš„ (Synchronized),æ˜¯çº¿ç¨‹å®‰å…¨çš„ (thread safe)ï¼Œè€Œ ArrayListçš„æ–¹æ³•ä¸æ˜¯ï¼Œç”±äºçº¿ç¨‹çš„åŒæ­¥å¿…ç„¶è¦å½±å“æ€§èƒ½ï¼Œå› æ­¤ ,ArrayListçš„æ€§èƒ½æ¯”Vectorå¥½ã€‚
2 å½“ Vectoræˆ– ArrayListä¸­çš„å…ƒç´ è¶…è¿‡å®ƒçš„åˆå§‹å¤§å°æ—¶ ,Vectorä¼šå°†å®ƒçš„å®¹é‡ç¿»å€ ,è€Œ ArrayListåªå¢åŠ 50%çš„å¤§å°ï¼Œè¿™æ · ,ArrayListå°±æœ‰åˆ©äºèŠ‚çº¦å†…å­˜ç©ºé—´ã€‚

#### linkedlist& ArrayList

ArrayList é‡‡ç”¨çš„æ˜¯æ•°ç»„å½¢å¼æ¥ä¿å­˜å¯¹è±¡çš„ï¼Œè¿™ç§æ–¹å¼å°†å¯¹è±¡æ”¾åœ¨è¿ç»­çš„ä½ç½®ä¸­ï¼Œæ‰€ä»¥æœ€å¤§çš„ç¼ºç‚¹å°±æ˜¯æ’å…¥åˆ é™¤æ—¶éå¸¸éº»çƒ¦

LinkedList é‡‡ç”¨çš„å°†å¯¹è±¡å­˜æ”¾åœ¨ç‹¬ç«‹çš„ç©ºé—´ä¸­ï¼Œè€Œä¸”åœ¨æ¯ä¸ªç©ºé—´ä¸­è¿˜ä¿å­˜ä¸‹ä¸€ä¸ªé“¾æ¥çš„ç´¢å¼• ä½†æ˜¯ç¼ºç‚¹å°±æ˜¯æŸ¥æ‰¾éå¸¸éº»çƒ¦ è¦ä¸›ç¬¬ä¸€ä¸ªç´¢å¼•å¼€å§‹



### å¤šçº¿ç¨‹å®ç°æ–¹å¼Threadå’ŒRunnableçš„åŒºåˆ«ï¼Ÿ

#### çº¿ç¨‹å®ç°ä»‹ç»

åœ¨javaä¸­å¯æœ‰ä¸¤ç§æ–¹å¼å®ç°å¤šçº¿ç¨‹ï¼Œä¸€ç§æ˜¯ç»§æ‰¿Threadç±»ï¼Œä¸€ç§æ˜¯å®ç°Runnableæ¥å£ï¼›

Threadç±»æ˜¯åœ¨java.langåŒ…ä¸­å®šä¹‰çš„ã€‚ä¸€ä¸ªç±»åªè¦ç»§æ‰¿äº†Threadç±»åŒæ—¶è¦†å†™äº†æœ¬ç±»ä¸­çš„run()æ–¹æ³•å°±å¯ä»¥å®ç°å¤šçº¿ç¨‹æ“ä½œäº†ï¼Œä½†æ˜¯ä¸€ä¸ªç±»åªèƒ½ç»§æ‰¿ä¸€ä¸ªçˆ¶ç±»ï¼Œè¿™æ˜¯æ­¤æ–¹æ³•çš„å±€é™ã€‚

ä¸‹é¢çœ‹ä¾‹å­ï¼š

```java
package org.thread.demo;
class MyThread extends Thread{
	private String name;
	
	public MyThread(String name) {
	super();
	this.name = name;
	}

	public void run(){
        for(int i=0;i<10;i++){
        System.out.println("çº¿ç¨‹å¼€å§‹ï¼š"+this.name+",i="+i);
        }
	}
}

package org.thread.demo;
public class ThreadDemo01 {
	public static void main(String[] args) {
        MyThread mt1=new MyThread("çº¿ç¨‹a");
        MyThread mt2=new MyThread("çº¿ç¨‹b");
        mt1.run();
        mt2.run();
	}
}
```

ä½†æ˜¯ï¼Œæ­¤æ—¶ç»“æœå¾ˆæœ‰è§„å¾‹ï¼Œå…ˆç¬¬ä¸€ä¸ªå¯¹è±¡æ‰§è¡Œï¼Œç„¶åç¬¬äºŒä¸ªå¯¹è±¡æ‰§è¡Œï¼Œå¹¶æ²¡æœ‰ç›¸äº’è¿è¡Œã€‚åœ¨JDKçš„æ–‡æ¡£ä¸­å¯ä»¥å‘ç°ï¼Œ**ä¸€æ—¦è°ƒç”¨start()æ–¹æ³•ï¼Œåˆ™ä¼šé€šè¿‡JVMæ‰¾åˆ°run()æ–¹æ³•**ã€‚ä¸‹é¢å¯åŠ¨start()æ–¹æ³•å¯åŠ¨çº¿ç¨‹ï¼š

```java
package org.thread.demo;
public class ThreadDemo01 {
	public static void main(String[] args) {
	MyThread mt1=new MyThread("çº¿ç¨‹a");
	MyThread mt2=new MyThread("çº¿ç¨‹b");
	mt1.start();
	mt2.start();
	}
};
```

è¿™æ ·ç¨‹åºå¯ä»¥æ­£å¸¸å®Œæˆäº¤äº’å¼è¿è¡Œã€‚é‚£ä¹ˆä¸ºå•¥éè¦ä½¿ç”¨start();æ–¹æ³•å¯åŠ¨å¤šçº¿ç¨‹å‘¢ï¼Ÿ
åœ¨JDKçš„å®‰è£…è·¯å¾„ä¸‹ï¼Œsrc.zipæ˜¯å…¨éƒ¨çš„javaæºç¨‹åºï¼Œé€šè¿‡æ­¤ä»£ç æ‰¾åˆ°Threadä¸­çš„start()æ–¹æ³•çš„å®šä¹‰ï¼Œå¯ä»¥å‘ç°æ­¤æ–¹æ³•ä¸­ä½¿ç”¨äº†**private native void start0()**;

å…¶ä¸­**nativeå…³é”®å­—**è¡¨ç¤ºå¯ä»¥è°ƒç”¨æ“ä½œç³»ç»Ÿçš„åº•å±‚å‡½æ•°ï¼Œé‚£ä¹ˆè¿™æ ·çš„æŠ€æœ¯æˆä¸ºJNIæŠ€æœ¯ï¼ˆjava Native Interfaceï¼‰

#### Runnableæ¥å£

åœ¨å®é™…å¼€å‘ä¸­ä¸€ä¸ªå¤šçº¿ç¨‹çš„æ“ä½œå¾ˆå°‘ä½¿ç”¨Threadç±»ï¼Œè€Œæ˜¯é€šè¿‡Runnableæ¥å£å®Œæˆã€‚

```java
public interface Runnable{
	public void run();
}
```

ä¾‹å­ï¼š

```java
package org.runnable.demo;
class MyThread implements Runnable{
	private String name;
	public MyThread(String name) {
	this.name = name;
	}
    
	public void run(){
        for(int i=0;i<100;i++){
        System.out.println("çº¿ç¨‹å¼€å§‹ï¼š"+this.name+",i="+i);
        }
	}
};
```



ä½†æ˜¯åœ¨ä½¿ç”¨Runnableå®šä¹‰çš„å­ç±»ä¸­æ²¡æœ‰start()æ–¹æ³•ï¼Œåªæœ‰Threadç±»ä¸­æ‰æœ‰ã€‚æ­¤æ—¶è§‚å¯ŸThreadç±»ï¼Œæœ‰ä¸€ä¸ªæ„é€ æ–¹æ³•ï¼špublic Thread(Runnable targer)æ­¤æ„é€ æ–¹æ³•æ¥å—Runnableçš„å­ç±»å®ä¾‹ï¼Œä¹Ÿå°±æ˜¯è¯´å¯ä»¥é€šè¿‡Threadç±»æ¥å¯åŠ¨Runnableå®ç°çš„å¤šçº¿ç¨‹ã€‚ï¼ˆstart()å¯ä»¥åè°ƒç³»ç»Ÿçš„èµ„æºï¼‰ï¼š

```java
package org.runnable.demo;

import org.runnable.demo.MyThread;

public class ThreadDemo01 {
public static void main(String[] args) {
	MyThread mt1=new MyThread("çº¿ç¨‹a");
	MyThread mt2=new MyThread("çº¿ç¨‹b");
	new Thread(mt1).start();
	new Thread(mt2).start();
	}
}
```

ä¸¤ç§å®ç°æ–¹å¼çš„åŒºåˆ«å’Œè”ç³»ï¼š

åœ¨ç¨‹åºå¼€å‘ä¸­åªè¦æ˜¯å¤šçº¿ç¨‹è‚¯å®šæ°¸è¿œä»¥å®ç°Runnableæ¥å£ä¸ºä¸»ï¼Œå› ä¸ºå®ç°Runnableæ¥å£ç›¸æ¯”ç»§æ‰¿Threadç±»æœ‰å¦‚ä¸‹å¥½å¤„ï¼š

> - é¿å…ç‚¹ç»§æ‰¿çš„å±€é™ï¼Œä¸€ä¸ªç±»å¯ä»¥ç»§æ‰¿å¤šä¸ªæ¥å£ã€‚
> - é€‚åˆäºèµ„æºçš„å…±äº«

ä»¥å–ç¥¨ç¨‹åºä¸ºä¾‹ï¼Œé€šè¿‡Threadç±»å®Œæˆï¼š
```java
package org.demo.dff;

class MyThread extends Thread{

private int ticket=10;

    public void run(){
        for(int i=0;i<20;i++){
            if(this.ticket>0){
            System.out.println("å–ç¥¨ï¼šticket"+this.ticket--);
            }
    	}
	}
};
```

ä¸‹é¢é€šè¿‡ä¸‰ä¸ªçº¿ç¨‹å¯¹è±¡ï¼ŒåŒæ—¶å–ç¥¨ï¼š

```java
package org.demo.dff;

public class ThreadTicket {
    public static void main(String[] args) {

        MyThread mt1=new MyThread();
        MyThread mt2=new MyThread();
        MyThread mt3=new MyThread();

        mt1.start();//æ¯ä¸ªçº¿ç¨‹éƒ½å„å–äº†10å¼ ï¼Œå…±å–äº†30å¼ ç¥¨
        mt2.start();//ä½†å®é™…åªæœ‰10å¼ ç¥¨ï¼Œæ¯ä¸ªçº¿ç¨‹éƒ½å–è‡ªå·±çš„ç¥¨
        mt3.start();//æ²¡æœ‰è¾¾åˆ°èµ„æºå…±äº«

    }
}
```

å®é™…åªæœ‰10å¼ è¡¨ï¼Œä½†æ˜¯ç”±äºä½™ç¥¨æ•°é‡ï¼ˆticketï¼‰æ²¡æœ‰å˜é‡å…±äº«ï¼Œæ‰€ä»¥å–å‡ºäº†30å¼ ã€‚

å¦‚æœç”¨Runnableå°±å¯ä»¥å®ç°**èµ„æºå…±äº«**ï¼Œä¸‹é¢çœ‹ä¾‹å­ï¼š

```java
    package org.demo.runnable;

    class MyThread implements Runnable {
        private int ticket = 10;

        public void run() {
            for (int i = 0; i < 20; i++) {
                if (this.ticket > 0) {
                    System.out.println("å–ç¥¨ï¼šticket" + this.ticket--);
                }
            }
        }
    };
     
    package org.demo.runnable;

    public class RunnableTicket {
        public static void main(String[] args) {
            MyThread mt = new MyThread();
            new Thread(mt).start();//åŒä¸€ä¸ªmtï¼Œä½†æ˜¯åœ¨Threadä¸­å°±ä¸å¯ä»¥ï¼Œå¦‚æœç”¨åŒä¸€
            new Thread(mt).start();//ä¸ªå®ä¾‹åŒ–å¯¹è±¡mtï¼Œå°±ä¼šå‡ºç°å¼‚å¸¸
            new Thread(mt).start();
        }
    };
```

è™½ç„¶ç°åœ¨ç¨‹åºä¸­æœ‰ä¸‰ä¸ªçº¿ç¨‹ï¼Œä½†æ˜¯ä¸€å…±å–äº†10å¼ ç¥¨ï¼Œä¹Ÿå°±æ˜¯è¯´ä½¿ç”¨**Runnableå®ç°å¤šçº¿ç¨‹å¯ä»¥è¾¾åˆ°èµ„æºå…±äº«ç›®çš„**ã€‚



### ä¸€ä¸ªHADOOPç¯å¢ƒï¼Œæ•´åˆäº†HBASEå’ŒHIVEï¼Œæ˜¯å¦æœ‰å¿…è¦ç»™HDFSå’ŒHBASEéƒ½åˆ†åˆ«é…ç½®å‹ç¼©ç­–ç•¥ï¼Ÿè¯·ç»™å‡ºå¯¹å‹ç¼©ç­–ç•¥çš„å»ºè®®ã€‚

hdfsåœ¨å­˜å‚¨çš„æ—¶å€™ä¸ä¼šå°†æ•°æ®è¿›è¡Œå‹ç¼©ï¼Œå¦‚æœæƒ³è¿›è¡Œå‹ç¼©ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å‘ hdfsä¸Šä¼ æ•°æ®çš„æ—¶å€™è¿›è¡Œå‹ç¼©ã€‚

#### 1. é‡‡ç”¨å‹ç¼©æµ

```java
//å‹ç¼©æ–‡ä»¶
public static void compress(String codecClassName) throws Exception{
	Class<?> codecClass = Class.forName(codecClassName);
	Configuration conf = new Configuration();
	FileSystem fs = FileSystem.get(conf);
	CompressionCodec codec = (CompressionCodec)ReflectionUtils.newInstance(codecClass, conf);
	
    //æŒ‡å®šå‹ç¼©æ–‡ä»¶è·¯å¾„
	FSDataOutputStream outputStream = fs.create(new Path("/user/hadoop/text.gz"));
	
    //æŒ‡å®šè¦è¢«å‹ç¼©çš„æ–‡ä»¶è·¯å¾„
	FSDataInputStream in = fs.open(new Path("/user/hadoop/aa.txt"));

    //åˆ›å»ºå‹ç¼©è¾“å‡ºæµ
	CompressionOutputStream out = codec.createOutputStream(outputStream);
	IOUtils.copyBytes(in, out, conf);
	IOUtils.closeStream(in);
    IOUtils.closeStream(out);
}
```



#### 2.é‡‡ç”¨åºåˆ—åŒ–æ–‡ä»¶

```java
public void testSeqWrite() throws Exception {
    
	Configuration conf = new Configuration();// åˆ›å»ºé…ç½®ä¿¡æ¯
	conf.set("fs.default.name", "hdfs://master:9000");// hdfsé»˜è®¤è·¯å¾„
	conf.set("hadoop.job.ugi", "hadoop,hadoop");// ç”¨æˆ·å’Œç»„ä¿¡æ¯
    
	String uriin = "hdfs://master:9000/ceshi2/";// æ–‡ä»¶è·¯å¾„
	FileSystem fs = FileSystem.get(URI.create(uriin), conf);// åˆ›å»ºfilesystem
	Path path = new Path("hdfs://master:9000/ceshi3/test.seq");// æ–‡ä»¶å
    
	IntWritable k = new IntWritable();// keyï¼Œç›¸å½“äº int
	Text v = new Text();// valueï¼Œç›¸å½“äº String
	SequenceFile.Writer w = SequenceFile.createWriter(fs, conf, path,k.getClass(), v.getClass());// åˆ›å»º writer

    for (int i = 1; i < 100; i++) {// å¾ªç¯æ·»åŠ 
        k.set(i);
        v.set("abcd");
        w.append(k, v);
	}
    
	w.close();
	IOUtils.closeStream(w);// å…³é—­çš„æ—¶å€™ flush
	fs.close();
}
```

hbaseä¸ºåˆ—å­˜æ•°æ®åº“ï¼Œæœ¬èº«å­˜åœ¨å‹ç¼©æœºåˆ¶ï¼Œæ‰€ä»¥æ— éœ€è®¾è®¡ã€‚



### â¤ï¸ç®€è¿°Hbaseæ€§èƒ½ä¼˜åŒ–çš„æ€è·¯

1. åœ¨åº“è¡¨è®¾è®¡çš„æ—¶å€™ï¼Œå°½é‡è€ƒè™‘rowkeyå’Œcolumnfamilyçš„ç‰¹æ€§
2. è¿›è¡Œhbaseé›†ç¾¤çš„è°ƒä¼˜ï¼šè§hbaseè°ƒä¼˜



### ç®€è¿°Hbase filterçš„å®ç°åŸç†æ˜¯ä»€ä¹ˆï¼Ÿç»“åˆå®é™…é¡¹ç›®ç»éªŒï¼Œå†™å‡ºå‡ ä¸ªä½¿ç”¨filterçš„åœºæ™¯

hbaseçš„filteræ˜¯é€šè¿‡scanè®¾ç½®çš„ï¼Œæ‰€ä»¥æ˜¯åŸºäºscançš„æŸ¥è¯¢ç»“æœè¿›è¡Œè¿‡æ»¤

1ã€ åœ¨è¿›è¡Œè®¢å•å¼€å‘çš„æ—¶å€™ï¼Œæˆ‘ä»¬ä½¿ç”¨ rowkeyfilterè¿‡æ»¤å‡ºæŸä¸ªç”¨æˆ·çš„æ‰€æœ‰è®¢å•ã€‚
2ã€ åœ¨è¿›è¡Œäº‘ç¬”è®°å¼€å‘æ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨ rowkeyè¿‡æ»¤å™¨è¿›è¡Œredisæ•°æ®çš„æ¢å¤ã€‚



### ROWKEYçš„åç¼€åŒ¹é…æ€ä¹ˆå®ç°ï¼Ÿä¾‹å¦‚ROWKEYæ˜¯yyyyMMDD-UserIDå½¢å¼ï¼Œå¦‚UserIDä¸ºæ¡ä»¶æŸ¥è¯¢æ•°æ®ï¼Œæ€ä¹ˆå®ç°

ä½¿ç”¨rowkeyè¿‡æ»¤å™¨å®ç°



### ç®€è¿°Hiveä¸­çš„è™šæ‹Ÿåˆ—ä½œç”¨æ˜¯ä»€ä¹ˆï¼Œä½¿ç”¨å®ƒçš„æ³¨æ„äº‹é¡¹

```properties
Hiveæä¾›äº†ä¸‰ä¸ªè™šæ‹Ÿåˆ—ï¼š
INPUT__FILE__NAME
BLOCK__OFFSET__INSIDE__FILE
ROW__OFFSET__INSIDE__BLOCK

ä½†ROW__OFFSET__INSIDE__BLOCKé»˜è®¤æ˜¯ä¸å¯ç”¨çš„ï¼Œéœ€è¦è®¾ç½® hive.exec.rowoffsetä¸º trueæ‰å¯ä»¥ã€‚å¯ä»¥ç”¨æ¥æ’æŸ¥æœ‰é—®é¢˜çš„è¾“å…¥æ•°æ®ã€‚
INPUT__FILE__NAME, mapperä»»åŠ¡çš„è¾“å‡ºæ–‡ä»¶åã€‚
BLOCK__OFFSET__INSIDE__FILE, å½“å‰å…¨å±€æ–‡ä»¶çš„åç§»é‡ã€‚å¯¹äºå—å‹ç¼©æ–‡ä»¶ï¼Œå°±æ˜¯å½“å‰å—çš„æ–‡ä»¶åç§»é‡ï¼Œå³å½“å‰å—çš„ ç¬¬ä¸€ä¸ªå­—èŠ‚åœ¨æ–‡ä»¶ä¸­çš„åç§»é‡ã€‚

hive> SELECT INPUT__FILE__NAME, BLOCK__OFFSET__INSIDE__FILE, line
> FROM hive_text WHERE line LIKE '%hive%' LIMIT 2;
har://file/user/hive/warehouse/hive_text/folder=docs/
data.har/user/hive/warehouse/hive_text/folder=docs/README.txt 2243
har://file/user/hive/warehouse/hive_text/folder=docs/
data.har/user/hive/warehouse/hive_text/folder=docs/README.txt 3646
```



### å¦‚æœè¦å­˜å‚¨æµ·é‡çš„å°æ–‡ä»¶ï¼ˆå¤§å°éƒ½æ˜¯å‡ ç™¾K~å‡ Mï¼‰ï¼Œè¯·ç®€è¿°è‡ªå·±çš„è®¾è®¡æ–¹æ¡ˆ

1. å°†å°æ–‡ä»¶è¾¾æˆharæ–‡ä»¶å­˜å‚¨
2. å°†å°æ–‡ä»¶åºåˆ—åŒ–åˆ°hdfsä¸­



### æœ‰ä¸¤ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼Œæ–‡ä»¶ä¸­çš„æ•°æ®æŒ‰è¡Œå­˜æ”¾ï¼Œè¯·ç¼–å†™MapReduceç¨‹åºï¼Œæ‰¾åˆ°ä¸¤ä¸ªæ–‡ä»¶ä¸­å½¼æ­¤ä¸ç›¸åŒçš„è¡Œ

å†™ä¸ªmapreduceé“¾ç”¨ä¾èµ–å…³ç³»ï¼Œä¸€å…±ä¸‰ä¸ªmapreduceï¼Œç¬¬ä¸€ä¸ªå¤„ç†ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼Œç¬¬äºŒä¸ªå¤„ç†ç¬¬äºŒä¸ªæ–‡ä»¶ï¼Œç¬¬ä¸‰ä¸ªå¤„ç†å‰ä¸¤ä¸ªçš„è¾“å‡ºç»“æœã€‚ç¬¬ä¸€ä¸ªmapreduceå°†æ–‡ä»¶å»é‡ï¼Œç¬¬äºŒä¸ªmapreduceä¹Ÿå°†æ–‡ä»¶å»é‡ï¼Œç¬¬ä¸‰ä¸ªåšwordcountï¼Œwordcountä¸º1çš„ç»“æœå°±æ˜¯ä¸åŒçš„ã€‚



### mapreduceæ‰¾å…±åŒæœ‹å‹

mapreduceæ‰¾å…±åŒæœ‹å‹ï¼Œæ•°æ®æ ¼å¼å¦‚ä¸‹ï¼š

1. A B C D E F
2. B A C D E
3. C A B E
4. D A B E
5. E A B C D
6. F A

ç¬¬ä¸€ä¸ªå­—æ¯è¡¨ç¤ºæœ¬äººï¼Œå…¶ä»–çš„æ˜¯ä»–çš„æœ‹å‹ï¼Œæ‰¾å‡ºæœ‰å…±åŒæœ‹å‹çš„äººï¼Œå’Œå…±åŒæœ‹å‹æ˜¯è°ã€‚



æ€è·¯ï¼šä¾‹å¦‚Aï¼Œä»–çš„æœ‹å‹æ˜¯B\C\D\E\Fï¼Œé‚£ä¹ˆBCçš„å…±åŒæœ‹å‹å°±æ˜¯Aã€‚æ‰€ä»¥å°†BCä½œä¸ºkeyï¼Œå°†Aä½œä¸ºvalueï¼Œåœ¨mapç«¯è¾“å‡ºå³å¯ï¼å…¶ä»–çš„æœ‹å‹å¾ªç¯å¤„ç†ã€‚

```java
import java.io.IOException;
import java.util.Set;
import java.util.StringTokenizer;
import java.util.TreeSet;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.Mapper.Context;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

public class FindFriend {

    public static class ChangeMapper extends Mapper<Object, Text, Text,Text>{
        @Override
        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
        StringTokenizer itr = new StringTokenizer(value.toString());
        Text owner = new Text();
        Set<String> set = new TreeSet<String>();
        owner.set(itr.nextToken());

        while (itr.hasMoreTokens()) {
            set.add(itr.nextToken());
         }
        
        //ä»keyä¸­æŸ¥æ‰¾å…±åŒæœ‹å‹
        String[] friends = new String[set.size()];
        friends = set.toArray(friends);
        for(int i=0;i<friends.length;i++){
            for(int j=i+1;j<friends.length;j++){
                String outputkey = friends[i]+friends[j];
                context.write(new Text(outputkey),owner);
            }
        }
    }
        
}

public static class FindReducer extends Reducer<Text,Text,Text,Text>{
	public void reduce(Text key, Iterable<Text> values,Context context) throws IOException,InterruptedException {
	String commonfriends ="";
       
        //è¾“å‡ºå…±åŒæœ‹å‹
        for (Text val : values) {
            if(commonfriends == ""){
                commonfriends = val.toString();
                }else{
                commonfriends = commonfriends+":"+val.toString();
                }
            }

        context.write(key, new Text(commonfriends));
        }
	}


public static void main(String[] args) throws IOException,InterruptedException, ClassNotFoundException {

	Configuration conf = new Configuration();
	String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
	if (otherArgs.length < 2) {
		System.err.println("args error");
		System.exit(2);
	}
    
    Job job = new Job(conf, "word count");
    job.setJarByClass(FindFriend.class);
    job.setMapperClass(ChangeMapper.class);
    job.setCombinerClass(FindReducer.class);
    job.setReducerClass(FindReducer.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(Text.class);
    
    for (int i = 0; i < otherArgs.length 1; ++i) {
    	FileInputFormat.addInputPath(job, new Path(otherArgs[i]));
    }
	FileOutputFormat.setOutputPath(job,new Path(otherArgs[otherArgs.length 1]));
	System.exit(job.waitForCompletion(true) ? 0 : 1);
}
```

ç»“æœï¼š
```java
1. AB E:C:D
2. AC E:B
3. AD B:E
4. AE C:B:D
5. BC A:E
6. BD A:E
7. BE C:D:A
8. BF A
9. CD E:A:B
10. CE A:B
11. CF A
12. DE B:A
13. DF A
14. EF A
```



### åŸºç«™é€—ç•™æ—¶é—´åˆå¹¶

ä½¿ç”¨Hiveæˆ–è‡ªå®šä¹‰MRå®ç°å¦‚ä¸‹é€»è¾‘

| product_no  | lac_id | moment | start_time                    | user_id | county_id | staytime | city_id |
| ----------- | ------ | ------ | ----------------------------- | ------- | --------- | -------- | ------- |
| 13429100031 | 22554  | 8      | 2013-03-11 08:55:19.151754088 | 571     | 571       | 282      | 571     |
| 13429100082 | 22540  | 8      | 2013-03-11 08:58:20.152622488 | 571     | 571       | 270      | 571     |
| 13429100082 | 22691  | 8      | 2013-03-11 08:56:37.149593624 | 571     | 571       | 103      | 571     |
| 13429100087 | 22750  | 8      | 2013-03-11 08:56:51.139539816 | 571     | 571       | 220      | 571     |
| 13429100087 | 22540  | 8      | 2013-03-11 08:55:45.150276800 | 571     | 571       | 66       | 571     |
| 13429100082 | 22540  | 8      | 2013-03-11 08:55:38.140225200 | 571     | 571       | 133      | 571     |
| 13429100140 | 26642  | 9      | 2013-03-11 09:02:19.151754088 | 571     | 571       | 18       | 571     |
| 13429100082 | 22691  | 8      | 2013-03-11 08:57:32.151754088 | 571     | 571       | 287      | 571     |

**å­—æ®µè§£é‡Š**

product_noï¼šç”¨æˆ·æ‰‹æœºå·ï¼›

lac_idï¼šç”¨æˆ·æ‰€åœ¨åŸºç«™ï¼›

start_timeï¼šç”¨æˆ·åœ¨æ­¤åŸºç«™çš„å¼€å§‹æ—¶é—´ï¼›

staytimeï¼šç”¨æˆ·åœ¨æ­¤åŸºç«™çš„é€—ç•™æ—¶é—´ã€‚



**éœ€æ±‚**

æ ¹æ® lac_id å’Œ start_time çŸ¥é“ç”¨æˆ·å½“æ—¶çš„ä½ç½®ï¼Œæ ¹æ® staytime çŸ¥é“ç”¨æˆ·å„ä¸ªåŸºç«™çš„é€—ç•™æ—¶é•¿ã€‚æ ¹æ®è½¨è¿¹åˆå¹¶è¿ç»­åŸºç«™çš„staytimeã€‚
æœ€ç»ˆå¾—åˆ°æ¯ä¸€ä¸ªç”¨æˆ·æŒ‰æ—¶é—´æ’åºåœ¨æ¯ä¸€ä¸ªåŸºç«™é©»ç•™æ—¶é•¿ã€‚



**æœŸæœ›ï¼š**

| 13429100082 | 22540 | 8    | 2013-03-11 08:58:20.152622488 | 571  | 571  | 270  | 571  |
| ----------- | ----- | ---- | ----------------------------- | ---- | ---- | ---- | ---- |
| 13429100082 | 22691 | 8    | 2013-03-11 08:56:37.149593624 | 571  | 571  | 103  | 571  |
| 13429100082 | 22540 | 8    | 2013-03-11 08:55:38.140225200 | 571  | 571  | 133  | 571  |
| 13429100087 | 22705 | 8    | 2013-03-11 08:56:51.139539816 | 571  | 571  | 220  | 571  |
| 13429100087 | 22540 | 8    | 2013-03-11 08:55:45.150276800 | 571  | 571  | 66   | 571  |

**æ€è·¯ï¼š**
å°†æ•°æ®å¯¼å…¥hiveè¡¨ä¸­ï¼ŒæŸ¥è¯¢æ—¶ï¼Œç”¨ç”µè¯å·ç å’Œæ—¶é—´æ’åºå³å¯ï¼



### è„šæœ¬æ›¿æ¢

è¯·éšæ„ä½¿ç”¨å„ç§ç±»å‹çš„è„šæœ¬è¯­è¨€å®ç°: æ‰¹é‡å°†æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ä¸­çš„`$HADOOP_HOOME$/home/ocetl/app/hadoop`æ›¿æ¢æˆ`/home/ocetl/app/hadoop`

è„šæœ¬ï¼šéšæ„å‘½åä¸ºaaa.sh

```shell
#!/bin/bash
ls $1 | while read line
do
sed i 's,\$HADOOP_HOME\$,\/home\/aa,g' $1$line
echo $1$line
done
```

è„šæœ¬æ‰§è¡Œå‘½ä»¤ï¼šæ›¿æ¢
/home/hadoop/test/ä¸‹çš„æ‰€æœ‰æ–‡ä»¶

```shell
./aaa.sh /home/hadoop/test/
```



### ä¸€é”®æ‰§è¡Œ

å‡è®¾æœ‰ 10 å°ä¸»æœºï¼ŒH1 åˆ° H10ï¼Œåœ¨å¼€å¯ SSH äº’ä¿¡çš„æƒ…å†µä¸‹ï¼Œç¼–å†™ä¸€ä¸ªæˆ–å¤šä¸ªè„šæœ¬å®ç°åœ¨æ‰€æœ‰çš„è¿œç¨‹ä¸»æœºä¸Šæ‰§è¡Œè„šæœ¬çš„åŠŸèƒ½

ä¾‹å¦‚ï¼šrunRemoteCmd.sh "ls-l"

è„šæœ¬ï¼š
vi runRemoteCmd.sh

```shell
#!/bin/bash
$1
ssh q hadoop@slave1 "$1"
ssh q hadoop@slave2 "$1"
```

æ‰§è¡Œå‘½ä»¤

```shell
./runRemoteCmd.sh "ls l"
```



## é¢è¯•ç®€ç­”

1. #### â¤ï¸è®²è§£ä¸€ä¸‹ MapReduce çš„ä¸€äº›åŸºæœ¬æµç¨‹

   ä»»åŠ¡æäº¤æµç¨‹ï¼Œä»»åŠ¡è¿è¡Œæµç¨‹

   

2. ä½ ä»¬æ•°æ®åº“æ€ä¹ˆå¯¼å…¥ hive çš„ ,æœ‰æ²¡æœ‰å‡ºç°é—®é¢˜ä½¿ç”¨

   sqoopå¯¼å…¥ï¼Œæˆ‘ä»¬å…¬å¸çš„æ•°æ®åº“ä¸­è®¾è®¡äº† textå­—æ®µï¼Œå¯¼è‡´å¯¼å…¥çš„æ—¶å€™å‡ºç°äº†ç¼“å­˜ä¸å¤Ÿçš„æƒ…å†µï¼ˆè§
   äº‘ç¬”è®°ï¼‰ï¼Œå¼€å§‹è§£å†³èµ·æ¥æ„Ÿè§‰å¾ˆæ£˜æ‰‹ï¼Œåæ¥æŸ¥çœ‹äº† sqoopçš„æ–‡æ¡£ï¼ŒåŠ ä¸Šäº† limitå±æ€§ï¼Œè§£å†³äº†ã€‚

   

3. å…¬å¸æŠ€æœ¯é€‰å‹å¯èƒ½åˆ©ç”¨ storm è¿›è¡Œå®æ—¶è®¡ç®— ,è®²è§£ä¸€ä¸‹ storm

   ä»stormçš„åº”ç”¨ï¼Œä»£ç ä¹¦å†™ï¼Œè¿è¡Œæœºåˆ¶è®²

   

4. é—®ä½  java é›†åˆç±»çš„æ•°æ®ç»“æ„ ,æ¯”å¦‚ hashmap
   çœ‹javaé¢è¯•å®å…¸

   

5. é—®ä½ çŸ¥ä¸çŸ¥é“ concurrent åŒ…ä¸‹çš„ä¸œè¥¿ ,ä¾‹å¦‚ concurrenthashmap
   çœ‹javaé¢è¯•å®å…¸



6. å…¬å¸æœ€è¿‘ä¸»è¦åœ¨è‡ªç„¶è¯­è¨€å­¦ä¹ å»å¼€å‘ ,æœ‰ æ²¡æœ‰æ¥è§¦è¿‡
   æ²¡æœ‰ç”¨è¿‡



## é¢è¯•é—®é¢˜1

1. ä»å‰åˆ°åä»ä½ æ•™è‚²èƒŒæ™¯ (å­¦è¿‡å“ªäº›è¯¾ )åˆ°å„ä¸ªé¡¹ç›®ä½ è´Ÿè´£çš„æ¨¡å— ,é—®çš„å¾ˆç»† (æœ¬ä»¥ä¸ºä»–æ˜¯ç‰©ç†å­¦åšå£« ,ä½†æ˜¯æ‰€æœ‰çš„æŠ€æœ¯éƒ½æ‡‚ )

  

2. **hadoop çš„ namenode å®•æœº ,æ€ä¹ˆè§£å†³**

  å…ˆåˆ†æå®•æœºåçš„æŸå¤±ï¼Œå®•æœºåç›´æ¥å¯¼è‡´clientæ— æ³•è®¿é—®ï¼Œå†…å­˜ä¸­çš„å…ƒæ•°æ®ä¸¢å¤±ï¼Œä½†æ˜¯ç¡¬ç›˜ä¸­çš„å…ƒæ•°æ®åº”è¯¥è¿˜å­˜åœ¨ï¼Œå¦‚æœåªæ˜¯èŠ‚ç‚¹æŒ‚äº†ï¼Œé‡å¯å³å¯ï¼Œå¦‚æœæ˜¯æœºå™¨æŒ‚äº†ï¼Œé‡å¯æœºå™¨åçœ‹èŠ‚ç‚¹æ˜¯å¦èƒ½é‡å¯ï¼Œä¸èƒ½é‡å¯å°±è¦æ‰¾åˆ°åŸå› ä¿®å¤äº†ã€‚ä½†æ˜¯æœ€ç»ˆçš„è§£å†³æ–¹æ¡ˆåº”è¯¥æ˜¯åœ¨è®¾è®¡é›†ç¾¤çš„åˆæœŸå°±è€ƒè™‘åˆ°è¿™ä¸ªé—®é¢˜ï¼Œåšnamenodeçš„ HAã€‚

  

3. ä¸€ä¸ª datanode å®•æœº ,æ€ä¹ˆä¸€ä¸ªæµç¨‹æ¢å¤
  Datanodeå®•æœºäº†åï¼Œå¦‚æœæ˜¯çŸ­æš‚çš„å®•æœºï¼Œå¯ä»¥å®ç°å†™å¥½è„šæœ¬ç›‘æ§ï¼Œå°†å®ƒå¯åŠ¨èµ·æ¥ã€‚å¦‚æœæ˜¯é•¿æ—¶é—´å®•æœºäº†ï¼Œé‚£ä¹ˆ datanodeä¸Šçš„æ•°æ®åº”è¯¥å·²ç»è¢«å¤‡ä»½åˆ°å…¶ä»–æœºå™¨äº†ï¼Œé‚£è¿™å° datanodeå°±æ˜¯ä¸€å°æ–°çš„ datanodeäº†ï¼Œåˆ é™¤ä»–çš„æ‰€æœ‰æ•°æ®æ–‡ä»¶å’ŒçŠ¶æ€æ–‡ä»¶ï¼Œé‡æ–°å¯åŠ¨ã€‚

4. Hbase çš„ç‰¹æ€§ ,ä»¥åŠä½ æ€ä¹ˆå»è®¾è®¡ rowkey å’Œ columnFamily ,æ€ä¹ˆå»å»ºä¸€ä¸ª table
   å› ä¸ºhbaseæ˜¯åˆ—å¼æ•°æ®åº“ï¼Œåˆ—éè¡¨ schemaçš„ä¸€éƒ¨åˆ†ï¼Œæ‰€ä»¥åœ¨è®¾è®¡åˆæœŸåªéœ€è¦è€ƒè™‘ rowkey å’ŒcolumnFamilyå³å¯ï¼Œ rowkeyæœ‰ä½ç½®ç›¸å…³æ€§ï¼Œæ‰€ä»¥å¦‚æœæ•°æ®æ˜¯ç»ƒä¹ æŸ¥è¯¢çš„ï¼Œæœ€å¥½å¯¹åŒç±»æ•°æ®åŠ ä¸€ä¸ªå‰ç¼€ï¼Œè€Œæ¯ä¸ª columnFamilyå®é™…ä¸Šåœ¨åº•å±‚æ˜¯ä¸€ä¸ªæ–‡ä»¶ï¼Œé‚£ä¹ˆæ–‡ä»¶è¶Šå°ï¼ŒæŸ¥è¯¢è¶Šå¿«ï¼Œæ‰€ä»¥è®²ç»å¸¸ä¸€èµ·æŸ¥è¯¢çš„åˆ—è®¾è®¡åˆ°ä¸€ä¸ªåˆ—ç°‡ï¼Œä½†æ˜¯åˆ—ç°‡ä¸å®œè¿‡å¤šã€‚

   

5. **â¤ï¸Redis,ä¼ ç»Ÿæ•°æ®åº“ ,hbase,hive æ¯ä¸ªä¹‹é—´çš„åŒºåˆ« (é—®çš„éå¸¸ç»† )**
   Redisæ˜¯ç¼“å­˜ï¼Œå›´ç»•ç€å†…å­˜å’Œç¼“å­˜è¯´
   Hbaseæ˜¯åˆ—å¼æ•°æ®åº“ï¼Œå­˜åœ¨ hdfsä¸Šï¼Œå›´ç»•ç€æ•°æ®é‡æ¥è¯´
   Hiveæ˜¯æ•°æ®ä»“åº“ï¼Œæ˜¯ç”¨æ¥åˆ†ææ•°æ®çš„ï¼Œä¸æ˜¯å¢åˆ æ”¹æŸ¥æ•°æ®çš„ã€‚



6. å…¬å¸ä¹‹åå€¾å‘ç”¨ spark å¼€å‘ ,ä½ ä¼šä¹ˆ (å°±ç”¨ javaä»£ç å»å†™ )

   ä¼šï¼Œsparkä½¿ç”¨ scalaå¼€å‘çš„ï¼Œåœ¨ scalaä¸­å¯ä»¥éšæ„ä½¿ç”¨ jdkçš„ç±»åº“ï¼Œå¯ä»¥ç”¨ javaå¼€å‘ï¼Œä½†æ˜¯æœ€å¥½ç”¨åŸç”Ÿçš„ scalaå¼€å‘ï¼Œå…¼å®¹æ€§å¥½ï¼Œ scalaæ›´çµæ´»ã€‚



## é¢è¯•é—®é¢˜2

1. ç¬”è¯• : javaåŸºç¡€ (åŸºæœ¬å…¨å¿˜ ,åšçš„å¾ˆçƒ‚ ,å¤ä¹ å¤§æ•°æ®è¿å•ä¾‹éƒ½å¿˜äº†æ€ä¹ˆå†™ )

   å¤ä¹ javaé¢è¯•å®å…¸

   

2. å¼€å§‹ä»‹ç»é¡¹ç›® ,ç›´æ¥ç”¨å¤§æ•°æ®é¡¹ç›®ä»‹ç» ,é¡¹ç›®ç»ç†ä¹Ÿæ‡‚å¤§æ•°æ®

   

3. Mapreduceä¸€äº›æµç¨‹ ,ç»è¿‡å“ªäº›æ­¥éª¤Map combiner partition sort copy sort grouping reduce

   ```sequence
   Map -->combiner:
   combiner -->partition:
   partition -->sort:
   sort -->copy:
   copy -->sort:
   sort -->grouping: 
   grouping-->reduce:
   ```

4. **â¤ï¸è¯´ä¸‹å¯¹ hadoop çš„ä¸€äº›ç†è§£ ,åŒ…æ‹¬å“ªäº›ç»„ä»¶**
   è¯¦è°ˆhadoopçš„åº”ç”¨ï¼ŒåŒ…æ‹¬çš„ç»„ä»¶åˆ†ä¸ºä¸‰ç±»ï¼Œåˆ†åˆ«è¯´æ˜ hdfs yarn mapreduce



5. è¯¦ç»†è®²è§£ä¸‹ä½ æµå¼å®æ—¶è®¡ç®—çš„é¡¹ç›®éƒ¨ç½²ä»¥åŠæ”¶é›†çš„ç»“æœæƒ…å†µ
   è®²è§£stormé›†ç¾¤çš„éƒ¨ç½²æ–¹æ¡ˆï¼Œé¡¹ç›®çš„å¤§å°ï¼Œä½¿ç”¨çš„ workeræ•°ï¼Œæ•°æ®æ”¶é›†åœ¨ hbaseæˆ–è€… hdfsï¼Œå¥½å¤„æ˜¯ä»€
   ä¹ˆ



6. ä½ çš„æ•°æ®åº“æ˜¯ä¸æ˜¯å¾ˆå¤§ä¹ˆ ,æœ‰æ²¡æœ‰åˆ†è¡¨ ,åˆ†åŒº ,ä½ æ˜¯æ€ä¹ˆå®ç°çš„

   æ•°æ®åº“çš„åˆ†è¡¨åœ¨è®¾è®¡åˆæœŸæ˜¯æŒ‰ç…§æœˆä»½è¿›è¡Œæ‹†åˆ†çš„ï¼Œä¸åŒçš„æœˆä»½æŸ¥è¯¢ä¸åŒçš„è¡¨ã€‚åˆ†åŒºæ²¡å¼„è¿‡ã€‚

   

7. å¼€å§‹é—® javaçš„ä¸€äº›ä¸œè¥¿ (ä»å„ç§æ¡†æ¶åŸç†åˆ°å„ç§å¤æ‚ SQL)



8. **ğŸ˜‹å¤šçº¿ç¨‹ ,å¹¶å‘ ,åƒåœ¾å›æ”¶æœºåˆ¶ ,æ•°æ®ç»“æ„ (é—®è¿™äº› ,åŸºæœ¬è§‰å¾—çœ‹ä½ æ˜¯ä¸æ˜¯é«˜çº§ç¨‹åºå‘˜äº† )**

   å¤šçº¿ç¨‹è¦çŸ¥é“æ“ä½œæ–¹å¼ï¼Œçº¿ç¨‹å®‰å…¨çš„é”ï¼Œå¹¶ä¸”è¦çŸ¥é“locké”
   åƒåœ¾å›æ”¶æœºåˆ¶éœ€è¦è¯¦ç»†äº†è§£ï¼ˆè§äº‘ç¬”è®°ï¼‰ï¼Œä¸»è¦ä»å†…å­˜åˆ’åˆ†ï¼Œåƒåœ¾å›æ”¶ä¸»è¦çš„å·¥ä½œåŒºåŸŸï¼Œåƒåœ¾å›æ”¶å™¨çš„
   ç§ç±»ï¼Œå„æœ‰ä»€ä¹ˆä¼˜ç¼ºç‚¹ï¼Œç”¨åœ¨å“ªé‡Œåˆé€‚ã€‚
   æ•°æ®ç»“æ„åŸºæœ¬çš„è¦çŸ¥é“ï¼Œå¤æ‚çš„å‚è€ƒç›¸å…³çš„ä¹¦ç±ã€‚



## é¢è¯•é—®é¢˜3

1. **BIå°ç»„çš„ 3ä¸ªå¹´è½»å­¦ç”Ÿä¸€èµ·æŠ€æœ¯é¢è¯• (ä¸€ä¸ªæ˜¯å—å¼€åšå£«ï¼‰**

2. æ•°æ®é‡å¤šå°‘ ,é›†ç¾¤è§„æ¨¡å¤šå¤§ ,å‹å·
   ä¸€èˆ¬ä¸­å‹çš„ç”µå•†æˆ–è€…äº’è”ç½‘ä¼ä¸šï¼Œæ—¥å¿—é‡æ¯å¤©åœ¨200 500Må·¦å³ï¼Œé›†ç¾¤è§„æ¨¡åœ¨ 30 50å°å·¦å³ï¼Œæœºå™¨ä¸€ èˆ¬ä¸º dellçš„ 2000å·¦å³çš„æœåŠ¡å™¨ï¼Œå‹å·ä¸å®šã€‚

   å¤§å‹çš„äº’è”ç½‘å…¬å¸æ®ç½‘ä¸Šèµ„æ–™æ˜¾ç¤ºï¼Œæ—¥å¿—é‡åœ¨GP PBä¸ç­‰ï¼Œé›†ç¾¤è§„æ¨¡åœ¨ 500 4000ä¸ç­‰ï¼Œç”šè‡³æ›´å¤šï¼Œæœºå™¨å‹å·ä¸ç¡®å®šã€‚

3. é¡¹ç›® ,mapreduce

   ä»‹ç»æ•´ä¸ªmapreduceé¡¹ç›®æµç¨‹ï¼Œæ•°æ®é‡‡é›† æ•°æ®èšåˆ æ•°æ®åˆ†æ æ•°æ®å±•ç¤ºç­‰

4. å®æ—¶æµå¼è®¡ç®—æ¡†æ¶ ,å‡ ä¸ªäºº ,å¤šé•¿æ—¶é—´ ,ç»†èŠ‚é—®é¢˜ ,åŒ…æ‹¬è®² flume ,kafka ,storm çš„å„ä¸ªçš„ç»„ä»¶ç»„æˆ ,ä½ è´Ÿè´£å“ªä¸€å— ,å¦‚æœéœ€è¦ä½ æ­å»ºä½ å¯ä»¥å®Œæˆä¹ˆ ?
5. ä½ è§‰å¾— spark å¯ä»¥å®Œå…¨æ›¿ä»£ hadoop ä¹ˆ ?



## é¢è¯•é—®é¢˜4

1. ä¸€äº›ä¼ ç»Ÿçš„ hadoop é—®é¢˜ ,mapreduce ä»–å°±é—® shuffle é˜¶æ®µ ,ä½ æ€ä¹ˆç†è§£çš„
  Shuffleæ„ä¹‰åœ¨äºå°†ä¸åŒ mapå¤„ç†åçš„æ•°æ®è¿›è¡Œåˆç†åˆ†é…ï¼Œè®© reduceå¤„ç†ï¼Œä»è€Œäº§ç”Ÿäº†æ’åºã€åˆ†åŒºã€‚

  

2. Mapreduce çš„ map æ•°é‡ å’Œ reduce æ•°é‡ æ€ä¹ˆç¡®å®š ,æ€ä¹ˆé…ç½®
  Mapæ— æ³•é…ç½®ï¼Œreduceéšä¾¿é…ç½®

  

3. å”¯ä¸€éš¾ä½æˆ‘çš„æ˜¯ä»–è¯´å®æ—¶è®¡ç®— ,storm å¦‚æœç¢°ä¸Šäº†å¤æ‚é€»è¾‘ ,éœ€è¦ç®—å¾ˆé•¿çš„æ—¶é—´ ,ä½ æ€ä¹ˆå»ä¼˜åŒ–
  æ‹†åˆ†å¤æ‚çš„ä¸šåŠ¡åˆ°å¤šä¸ª
  boltä¸­ï¼Œè¿™æ ·å¯ä»¥åˆ©ç”¨ boltçš„ treeå°†é€Ÿåº¦æå‡

  

4. Hive ä½ ä»¬ç”¨çš„æ˜¯å¤–éƒ¨è¡¨è¿˜æ˜¯å†…éƒ¨è¡¨ ,æœ‰æ²¡æœ‰å†™è¿‡ UDF(å½“ç„¶å¹è‡ªå·±å†™è¿‡äº† ),hive çš„ç‰ˆæœ¬
  å¤–éƒ¨è¡¨ï¼Œudf udafç­‰ï¼Œ hiveç‰ˆæœ¬ä¸º 1.0

  

5. Hadoop çš„ç‰ˆæœ¬
  å¦‚æœæ˜¯
  1.0ç‰ˆæœ¬å°±è¯´ 1.2ï¼Œå¦‚æœæ˜¯ 2.0ç‰ˆæœ¬ï¼Œå°±è¯´ 2.6æˆ–è€… 2.7
  1.2ä¸ºå®˜æ–¹ç¨³å®šç‰ˆæœ¬ï¼Œ 2.7ä¸ºå®˜æ–¹ç¨³å®šç‰ˆæœ¬ã€‚
  Apache Hadoop 2.7.1äºç¾å›½æ—¶é—´ 2015å¹´ 07æœˆ 06æ—¥æ­£å¼å‘å¸ƒï¼Œæœ¬ç‰ˆæœ¬å±äºç¨³å®šç‰ˆæœ¬ï¼Œæ˜¯è‡ª Hadoop 2.6.0ä»¥æ¥åˆä¸€ä¸ªç¨³å®šç‰ˆï¼ŒåŒæ—¶ä¹Ÿ æ˜¯ Hadoop 2.7.xç‰ˆæœ¬çº¿çš„ç¬¬ä¸€ä¸ªç¨³å®šç‰ˆæœ¬ï¼Œä¹Ÿæ˜¯ 2.7ç‰ˆæœ¬çº¿çš„ç»´æŠ¤ç‰ˆæœ¬ï¼Œå˜åŒ–ä¸å¤§ï¼Œä¸»è¦æ˜¯ä¿®å¤äº†ä¸€äº›æ¯”è¾ƒä¸¥é‡çš„ Bug

  

6. å®æ—¶æµå¼è®¡ç®— çš„ç»“æœå†…å®¹æœ‰å“ªäº› ,ä½ ä»¬éœ€è¦ç»Ÿè®¡å‡ºæ¥ä¹ˆ (æˆ‘å°±è¯´ highchartå±•ç¤º )
  ç®€å•ä»‹ç»æ—¥å¿—ç›‘æ§ã€é£æ§ç­‰ç»“æœå†…å®¹ï¼Œç»Ÿè®¡å‡ºæ¥æ˜¾ç¤ºåœ¨æŠ¥è¡¨æˆ–è€…é‚®ä»¶ä¸­ã€‚

  

7. å¼€å§‹é—® javaç›¸å…³ ,åŒ…æ‹¬ luecne,solr(å€’æ’ç´¢å¼•çš„åŸç† ),æ¡†æ¶å‘€ ,rediså‘€ã€‚

## â¤ï¸äº¬ä¸œå•†åŸ-å¤§æ•°æ®é¢è¯•

#### javaç¯‡

1. JVM,GC(ç®—æ³•ï¼Œæ–°ç”Ÿä»£ï¼Œè€å¹´ä»£)ï¼ŒJVMç»“æ„

2. hashcodeï¼ŒhashMapï¼Œlistï¼ŒhashSetï¼Œequalsï¼ˆç»“æ„åŸç†ï¼‰ï¼ŒA extends Bï¼ˆç±»çš„åŠ è½½é¡ºåºï¼‰

   1.çˆ¶ç±»é™æ€ä»£ç å—ï¼› 2.å­ç±»é™æ€ä»£ç å—ï¼› 3.çˆ¶ç±»éé™æ€ä»£ç å—ï¼› 4.çˆ¶ç±»æ„é€ å‡½æ•°ï¼› 5.å­ç±»éé™æ€ä»£ç å—ï¼› 6.å­ç±»æ„é€ å‡½æ•°ï¼›

3. å¤šçº¿ç¨‹ï¼Œä¸»çº¿ç¨‹ï¼Œæ¬¡çº¿ç¨‹ï¼Œå”¤é†’ï¼Œç¡çœ 

   ç•¥

4. å¸¸è§ç®—æ³•ï¼šå†’æ³¡ç®—æ³•ã€æ’åºç®—æ³•ï¼ŒäºŒåˆ†æŸ¥æ‰¾ï¼Œæ—¶é—´å¤æ‚åº¦

   ç•¥

   

#### Flumeç¯‡

1. æ•°æ®æ€ä¹ˆé‡‡é›†åˆ°Kafkaï¼Œå®ç°æ–¹å¼ ä½¿ç”¨å®˜æ–¹æä¾›çš„flumeKafkaæ’ä»¶ï¼Œæ’ä»¶çš„å®ç°æ–¹å¼æ˜¯è‡ªå®šä¹‰äº†flumeçš„sinkï¼Œå°†æ•°æ®ä»channleä¸­å–å‡ºï¼Œé€šè¿‡kafkaçš„producerå†™å…¥åˆ°kafkaä¸­ï¼Œå¯ä»¥è‡ªå®šä¹‰åˆ†åŒºç­‰ã€‚



3. flumeç®¡é“å†…å­˜ï¼Œflumeå®•æœºäº†æ•°æ®ä¸¢å¤±æ€ä¹ˆè§£å†³ 
   1. Flumeçš„channelåˆ†ä¸ºå¾ˆå¤šç§ï¼Œå¯ä»¥å°†æ•°æ®å†™å…¥åˆ°æ–‡ä»¶ 
   2. é˜²æ­¢éé¦–ä¸ªagentå®•æœºçš„æ–¹æ³•æ•°å¯ä»¥åšé›†ç¾¤æˆ–è€…ä¸»å¤‡



4. flumeé…ç½®æ–¹å¼ï¼Œflumeé›†ç¾¤ï¼ˆé—®çš„å¾ˆè¯¦ç»†ï¼‰ 

   Flumeçš„é…ç½®å›´ç»•ç€sourceã€channelã€sinkå™è¿°ï¼Œflumeçš„é›†ç¾¤æ˜¯åšåœ¨agentä¸Šçš„ï¼Œè€Œéæœºå™¨ä¸Šã€‚



5. flumeä¸é‡‡é›†Nginxæ—¥å¿—ï¼Œé€šè¿‡Logger4jé‡‡é›†æ—¥å¿—ï¼Œä¼˜ç¼ºç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ

   ä¼˜ç‚¹ï¼šNginxçš„æ—¥å¿—æ ¼å¼æ˜¯å›ºå®šçš„ï¼Œä½†æ˜¯ç¼ºå°‘sessionidï¼Œé€šè¿‡logger4jé‡‡é›†çš„æ—¥å¿—æ˜¯å¸¦æœ‰sessionidçš„ï¼Œè€Œsessionå¯ä»¥é€šè¿‡rediså…±äº«ï¼Œä¿è¯äº†é›†ç¾¤æ—¥å¿—ä¸­çš„åŒä¸€sessionè½åˆ°ä¸åŒçš„tomcatæ—¶ï¼ŒsessionIdè¿˜æ˜¯ä¸€æ ·çš„ï¼Œè€Œä¸”logger4jçš„æ–¹å¼æ¯”è¾ƒç¨³å®šï¼Œä¸ä¼šå®•æœºã€‚ 

   ç¼ºç‚¹ï¼šä¸å¤Ÿçµæ´»ï¼Œlogger4jçš„æ–¹å¼å’Œé¡¹ç›®ç»“åˆè¿‡äºç´§å¯†ï¼Œè€Œflumeçš„æ–¹å¼æ¯”è¾ƒçµæ´»ï¼Œæ‹”æ’å¼æ¯”è¾ƒå¥½ï¼Œä¸ä¼šå½±å“é¡¹ç›®æ€§èƒ½ã€‚



6. flumeå’Œkafkaé‡‡é›†æ—¥å¿—åŒºåˆ«ï¼Œé‡‡é›†æ—¥å¿—æ—¶ä¸­é—´åœäº†ï¼Œæ€ä¹ˆè®°å½•ä¹‹å‰çš„æ—¥å¿—ã€‚

   Flumeé‡‡é›†æ—¥å¿—æ˜¯é€šè¿‡æµçš„æ–¹å¼ç›´æ¥å°†æ—¥å¿—æ”¶é›†åˆ°å­˜å‚¨å±‚ï¼Œè€Œkafkaè¯•è®²æ—¥å¿—ç¼“å­˜åœ¨kafkaé›†ç¾¤ï¼Œå¾…åæœŸå¯ä»¥é‡‡é›†åˆ°å­˜å‚¨å±‚ã€‚ Flumeé‡‡é›†ä¸­é—´åœäº†ï¼Œå¯ä»¥é‡‡ç”¨æ–‡ä»¶çš„æ–¹å¼è®°å½•ä¹‹å‰çš„æ—¥å¿—ï¼Œè€Œkafkaæ˜¯é‡‡ç”¨offsetçš„æ–¹å¼è®°å½•ä¹‹å‰çš„æ—¥å¿—ã€‚



#### Kafkaç¯‡

1. å®¹é”™æœºåˆ¶ 

   åˆ†åŒºå¤‡ä»½ï¼Œå­˜åœ¨ä¸»å¤‡partition

   

2. åŒä¸€topicä¸åŒpartitionåˆ†åŒº 

   ï¼Ÿï¼Ÿï¼Ÿï¼Ÿ



3. kafkaæ•°æ®æµå‘ 

   Producer â†’ leader partition â†’ follower partition(åŠæ•°ä»¥ä¸Š) â†’consumer

   

4. kafka+spark-streamingç»“åˆä¸¢æ•°æ®æ€ä¹ˆè§£å†³ï¼Ÿ

   spark streamingä»1.2å¼€å§‹æä¾›äº†æ•°æ®çš„é›¶ä¸¢å¤±ï¼Œæƒ³äº«å—è¿™ä¸ªç‰¹æ€§ï¼Œéœ€è¦æ»¡è¶³å¦‚ä¸‹æ¡ä»¶ï¼š 

   1. æ•°æ®è¾“å…¥éœ€è¦å¯é çš„sourceså’Œå¯é çš„receivers 
   2. åº”ç”¨metadataå¿…é¡»é€šè¿‡åº”ç”¨driver checkpoint
   3. WALï¼ˆwrite ahead logï¼‰



1. â€‹	**å¯é çš„ sourceså’Œ receivers** 

â€‹	spark streamingå¯ä»¥é€šè¿‡å¤šç§æ–¹å¼ä½œä¸ºæ•°æ®sourcesï¼ˆåŒ…æ‹¬kafkaï¼‰ï¼Œè¾“å…¥æ•°æ®é€šè¿‡receiversæ¥æ”¶ï¼Œé€šè¿‡replicationå­˜å‚¨äºsparkä¸­ï¼ˆä¸ºäº†faultoleranceï¼Œé»˜è®¤å¤åˆ¶åˆ°ä¸¤ä¸ªspark executorsï¼‰ï¼Œå¦‚æœæ•°æ®å¤åˆ¶å®Œæˆï¼Œreceiverså¯ä»¥çŸ¥é“ï¼ˆä¾‹å¦‚kafkaä¸­æ›´æ–°offsetsåˆ°zookeeperä¸­ï¼‰ã€‚è¿™æ ·å½“receiversåœ¨æ¥æ”¶æ•°æ®è¿‡ç¨‹ä¸­crashæ‰ï¼Œä¸ä¼šæœ‰æ•°æ®ä¸¢å¤±ï¼Œreceiversæ²¡æœ‰å¤åˆ¶çš„æ•°æ®ï¼Œå½“receiveræ¢å¤åé‡æ–°æ¥æ”¶ã€‚

![img](https://raw.githubusercontent.com/jacksu/utils4s/master/spark-knowledge/images/spark-streaming-kafka/spark-reliable-source-reliable-receiver.png)

2. **metadata checkpoint** 

â€‹	å¯é çš„sourceså’Œreceiversï¼Œå¯ä»¥ä½¿æ•°æ®åœ¨receiverså¤±è´¥åæ¢å¤ï¼Œç„¶è€Œåœ¨driverå¤±è´¥åæ¢å¤æ˜¯æ¯”è¾ƒå¤æ‚çš„ï¼Œä¸€ç§æ–¹æ³•æ˜¯é€šè¿‡checkpoint metadataåˆ°HDFSæˆ–è€…S3ã€‚metadataåŒ…æ‹¬ï¼š

- configuration 
- code 
- ä¸€äº›æ’é˜Ÿç­‰å¾…å¤„ç†ä½†æ²¡æœ‰å®Œæˆçš„ RDDï¼ˆä»…ä»…æ˜¯ metadataï¼Œè€Œä¸æ˜¯ data

![img](https://raw.githubusercontent.com/jacksu/utils4s/master/spark-knowledge/images/spark-streaming-kafka/spark-metadata-checkpointing.png)

è¿™æ ·å½“driverå¤±è´¥æ—¶ï¼Œå¯ä»¥é€šè¿‡metadata checkpointï¼Œé‡æ„åº”ç”¨ç¨‹åºå¹¶çŸ¥é“æ‰§è¡Œåˆ°é‚£ä¸ªåœ°æ–¹ã€‚



3. **æ•°æ®å¯èƒ½ä¸¢å¤±çš„åœºæ™¯**
   å¯é çš„sourceså’Œreceiversï¼Œä»¥åŠmetadata checkpointä¹Ÿä¸å¯ä»¥ä¿è¯æ•°æ®çš„ä¸ä¸¢å¤±ï¼Œä¾‹å¦‚ï¼š 
   1. ä¸¤ä¸ª executorå¾—åˆ°è®¡ç®—æ•°æ®ï¼Œå¹¶ä¿å­˜åœ¨ä»–ä»¬çš„å†…å­˜ä¸­
   2. receiversçŸ¥é“æ•°æ®å·²ç»è¾“å…¥
   3. executorså¼€å§‹è®¡ç®—æ•°æ®
   4. driverçªç„¶å¤±è´¥
   5. driverå¤±è´¥ï¼Œé‚£ä¹ˆ executorséƒ½ä¼šè¢« killæ‰
   6. å› ä¸º executorè¢« killæ‰ï¼Œé‚£ä¹ˆä»–ä»¬å†…å­˜ä¸­å¾—æ•°æ®éƒ½ä¼šä¸¢å¤±ï¼Œä½†æ˜¯è¿™äº›æ•°æ®ä¸å†è¢«å¤„ç†
   7. executorä¸­çš„æ•°æ®ä¸å¯æ¢å¤



4. **WAL**

   ä¸ºäº†é¿å…ä¸Šé¢æƒ…æ™¯çš„å‡ºç°ï¼Œspark streaming 1.2å¼•å…¥äº†WALã€‚æ‰€æœ‰æ¥æ”¶çš„æ•°æ®é€šè¿‡receiverså†™å…¥HDFSæˆ–è€…S3ä¸­checkpointç›®å½•ï¼Œè¿™æ ·å½“driverå¤±è´¥åï¼Œexecutorä¸­æ•°æ®ä¸¢å¤±åï¼Œå¯ä»¥é€šè¿‡checkpointæ¢å¤ã€‚

![img](https://raw.githubusercontent.com/jacksu/utils4s/master/spark-knowledge/images/spark-streaming-kafka/spark-wal.png)



5. **At Least Once**

å°½ç®¡WALå¯ä»¥ä¿è¯æ•°æ®é›¶ä¸¢å¤±ï¼Œä½†æ˜¯ä¸èƒ½ä¿è¯exactly-onceï¼Œä¾‹å¦‚ä¸‹é¢åœºæ™¯ï¼š 

- Receiversæ¥æ”¶å®Œæ•°æ®å¹¶ä¿å­˜åˆ°HDFSæˆ–S3 

- åœ¨æ›´æ–°offsetå‰ï¼Œreceiverså¤±è´¥äº†

![img](https://raw.githubusercontent.com/jacksu/utils4s/master/spark-knowledge/images/spark-streaming-kafka/spark-wall-at-least-once-delivery.png)

- Spark Streamingä»¥ä¸ºæ•°æ®æ¥æ”¶æˆåŠŸï¼Œä½†æ˜¯Kafkaä»¥ä¸ºæ•°æ®æ²¡æœ‰æ¥æ”¶æˆåŠŸï¼Œå› ä¸ºoffsetæ²¡æœ‰æ›´æ–°åˆ°zookeeper 
- éšåreceiveræ¢å¤äº† 
- ä»WALå¯ä»¥è¯»å–çš„æ•°æ®é‡æ–°æ¶ˆè´¹ä¸€æ¬¡ï¼Œå› ä¸ºä½¿ç”¨çš„kafka High-Levelæ¶ˆè´¹APIï¼Œä»zookeeperä¸­ä¿å­˜çš„offsetså¼€å§‹æ¶ˆè´¹



6. **WALçš„ç¼ºç‚¹**

é€šè¿‡ä¸Šé¢æè¿°ï¼ŒWALæœ‰ä¸¤ä¸ªç¼ºç‚¹ï¼š

- é™ä½äº† receiversçš„æ€§èƒ½ï¼Œå› ä¸ºæ•°æ®è¿˜è¦å­˜å‚¨åˆ° HDFSç­‰åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ
- å¯¹äºä¸€äº› resourcesï¼Œå¯èƒ½å­˜åœ¨é‡å¤çš„æ•°æ®ï¼Œæ¯”å¦‚ Kafkaï¼Œåœ¨ Kafkaä¸­å­˜åœ¨ä¸€ä»½æ•°æ®ï¼Œåœ¨ Spark Streamingä¹Ÿå­˜åœ¨ä¸€ä»½ï¼ˆä»¥ WALçš„å½¢å¼å­˜å‚¨åœ¨ hadoop APIå…¼å®¹çš„æ–‡ä»¶ç³»
  ç»Ÿä¸­ï¼‰



7. **Kafka direct API**

ä¸ºäº†WALçš„æ€§èƒ½æŸå¤±å’Œexactly-onceï¼Œspark streaming1.3ä¸­ä½¿ç”¨Kafka direct APIã€‚éå¸¸å·§å¦™ï¼ŒSpark driverè®¡ç®—ä¸‹ä¸ªbatchçš„offsetsï¼ŒæŒ‡å¯¼executoræ¶ˆè´¹å¯¹åº”çš„topicså’Œpartitionsã€‚æ¶ˆè´¹Kafkaæ¶ˆæ¯ï¼Œå°±åƒæ¶ˆè´¹æ–‡ä»¶ç³»ç»Ÿæ–‡ä»¶ä¸€æ ·ã€‚

![img](https://raw.githubusercontent.com/jacksu/utils4s/master/spark-knowledge/images/spark-streaming-kafka/spark-kafka-direct-api.png)

1. ä¸å†éœ€è¦kafka receiversï¼Œexecutorç›´æ¥é€šè¿‡Kafka APIæ¶ˆè´¹æ•°æ®
2. WALä¸å†éœ€è¦ï¼Œå¦‚æœä»å¤±è´¥æ¢å¤ï¼Œå¯ä»¥é‡æ–°æ¶ˆè´¹
3. exactly-onceå¾—åˆ°äº†ä¿è¯ï¼Œä¸ä¼šå†ä»WALä¸­é‡å¤è¯»å–æ•°æ®



8. **æ€»ç»“**

ä¸»è¦è¯´çš„æ˜¯spark streamingé€šè¿‡å„ç§æ–¹å¼æ¥ä¿è¯æ•°æ®ä¸ä¸¢å¤±ï¼Œå¹¶ä¿è¯exactly-onceï¼Œæ¯ä¸ªç‰ˆæœ¬éƒ½æ˜¯spark streamingè¶Šæ¥è¶Šç¨³å®šï¼Œè¶Šæ¥è¶Šå‘ç”Ÿäº§ç¯å¢ƒä½¿ç”¨å‘å±•ã€‚



5ã€kafkaä¸­å­˜å‚¨ç›®å½•data/dir.....topic1å’Œtopic2æ€ä¹ˆå­˜å‚¨çš„ï¼Œå­˜å‚¨ç»“æ„ï¼Œdata.....ç›®å½•ä¸‹æœ‰å¤šå°‘ä¸ªåˆ†åŒºï¼Œæ¯ä¸ªåˆ†åŒºçš„å­˜å‚¨æ ¼å¼æ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿ 

1. topicæ˜¯æŒ‰ç…§â€œä¸»é¢˜å-åˆ†åŒºâ€å­˜å‚¨çš„ 2. 
2. åˆ†åŒºä¸ªæ•°ç”±é…ç½®æ–‡ä»¶å†³å®š 3. 
3. æ¯ä¸ªåˆ†åŒºä¸‹æœ€é‡è¦çš„ä¸¤ä¸ªæ–‡ä»¶æ˜¯0000000000.logå’Œ000000.indexï¼Œ0000000.logä»¥é»˜è®¤1Gå¤§å°å›æ»šã€‚



#### Hiveç¯‡

1. hive partitionåˆ†åŒº 

   åˆ†åŒºè¡¨ï¼ŒåŠ¨æ€åˆ†åŒº



2. insert into å’Œ override writeåŒºåˆ«ï¼Ÿ

   insert intoï¼šå°†æŸä¸€å¼ è¡¨ä¸­çš„æ•°æ®å†™åˆ°å¦ä¸€å¼ è¡¨ä¸­ 

   override writeï¼šè¦†ç›–ä¹‹å‰çš„å†…å®¹ã€‚

   

3. å‡å¦‚ä¸€ä¸ªåˆ†åŒºçš„æ•°æ®ä¸»éƒ¨é”™è¯¯æ€ä¹ˆé€šè¿‡hivesqlåˆ é™¤hdfs

   alter table ptable drop partition (daytime='20140911',city='bj'); 

   å…ƒæ•°æ®ï¼Œæ•°æ®æ–‡ä»¶éƒ½åˆ é™¤ï¼Œä½†ç›®å½•daytime= 20140911è¿˜åœ¨



#### Stormç¯‡

1. å¼€å‘æµç¨‹ï¼Œå®¹é”™æœºåˆ¶

å¼€å‘æµç¨‹ï¼š 

1. å†™ä¸»ç±»ï¼ˆè®¾è®¡spoutå’Œboltçš„åˆ†å‘æœºåˆ¶ï¼‰ 2. 

2. å†™spoutæ”¶é›†æ•°æ® 3. 

3. å†™boltå¤„ç†æ•°æ®ï¼Œæ ¹æ®æ•°æ®é‡å’Œä¸šåŠ¡çš„å¤æ‚ç¨‹åº¦ï¼Œè®¾è®¡å¹¶è¡Œåº¦ã€‚ 

   å®¹é”™æœºåˆ¶ï¼šé‡‡ç”¨ackå’Œfailè¿›è¡Œå®¹é”™ï¼Œå¤±è´¥çš„æ•°æ®é‡æ–°å‘é€ã€‚



2. stormå’Œspark-streamingï¼šä¸ºä»€ä¹ˆç”¨stormä¸åŒspark-streaming

   

3. mrå’ŒsparkåŒºåˆ«ï¼Œæ€ä¹ˆç†è§£spark-rdd

Mræ˜¯æ–‡ä»¶æ–¹å¼çš„åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶ï¼Œæ˜¯å°†ä¸­é—´ç»“æœå’Œæœ€ç»ˆç»“æœè®°å½•åœ¨æ–‡ä»¶ä¸­ï¼Œmapå’Œreduceçš„æ•°æ®åˆ†å‘ä¹Ÿæ˜¯åœ¨æ–‡ä»¶ä¸­ã€‚ 

sparkæ˜¯å†…å­˜è¿­ä»£å¼çš„è®¡ç®—æ¡†æ¶ï¼Œè®¡ç®—çš„ä¸­é—´ç»“æœå¯ä»¥ç¼“å­˜å†…å­˜ï¼Œä¹Ÿå¯ä»¥ç¼“å­˜ç¡¬ç›˜ï¼Œä½†æ˜¯ä¸æ˜¯æ¯ä¸€æ­¥è®¡ç®—éƒ½éœ€è¦ç¼“å­˜çš„ã€‚ 

Spark-rddæ˜¯ä¸€ä¸ªæ•°æ®çš„åˆ†åŒºè®°å½•é›†åˆâ€¦â€¦â€¦â€¦â€¦â€¦

4. sqoopå‘½ä»¤

```shell
sqoop import --connect jdbc:mysql://192.168.56.204:3306/sqoop --username hive --password hive --table jobinfo --target-dir /sqoop/test7 --inline-lob-limit 16777216 --fields-terminated-by '\t' -m 2

sqoop create-hive-table --connect jdbc:mysql://192.168.56.204:3306/sqoop --table jobinfo --username hive --password hive --hive-table sqtest --fields-terminated-by "\t" --lines-terminated-by "\n";
```



#### Redisç¯‡

1. åŸºæœ¬æ“ä½œï¼Œå­˜å‚¨æ ¼å¼

   ç•¥



#### Mysqlç¯‡

1. mysqlé›†ç¾¤çš„åˆ†å¸ƒå¼äº‹åŠ¡

   äº¬ä¸œè‡ªä¸»å¼€å‘åˆ†å¸ƒå¼MYSQLé›†ç¾¤ç³»ç»Ÿ

   

2. mysqlæ€§èƒ½ä¼˜åŒ–ï¼ˆæ•°æ®æ–¹é¢ï¼‰

   æ•°æ®çš„åˆ†è¡¨ã€åˆ†åº“ã€åˆ†åŒº



#### Hadoopç¯‡

1. hadoop HA ä¸¤ä¸ªnamenodeå’Œzkä¹‹é—´çš„é€šä¿¡ï¼Œzkçš„é€‰ä¸¾æœºåˆ¶ï¼Ÿ

   HAæ˜¯é€šè¿‡å…ˆåè·å–zkçš„é”å†³å®šè°æ˜¯ä¸» 

   Zkçš„é€‰ä¸¾æœºåˆ¶ï¼Œæ¶‰åŠåˆ°å…¨æ–°æœºç¾¤çš„é€‰ä¸»å’Œæ•°æ®æ¢å¤çš„é€‰ä¸»

   

2. mrè¿è¡Œæœºåˆ¶

![image-20210907194545886](https://gitee.com/vicxsl/img/raw/master/img/1631015146340/image-20210907194545886.png)



3. yarnæµç¨‹

![image-20210907194606067](https://gitee.com/vicxsl/img/raw/master/img/1631015166515/image-20210907194606067.png)

1. ç”¨æˆ·å‘ YARN ä¸­æäº¤åº”ç”¨ç¨‹åºï¼Œ å…¶ä¸­åŒ…æ‹¬ ApplicationMaster ç¨‹åºã€å¯åŠ¨ApplicationMaster çš„å‘½ ä»¤ã€ç”¨æˆ·ç¨‹åºç­‰ã€‚
2. ResourceManager ä¸ºè¯¥åº”ç”¨ç¨‹åºåˆ†é…ç¬¬ä¸€ä¸ª Container å¹¶ä¸å¯¹åº”çš„ NodeManager é€šä¿¡ï¼Œè¦æ±‚å®ƒåœ¨è¿™ä¸ª Container ä¸­å¯åŠ¨åº”ç”¨ç¨‹åºçš„ ApplicationMaster 
3.  ApplicationMaster é¦–å…ˆå‘ ResourceManager æ³¨å†Œï¼Œ è¿™æ ·ç”¨æˆ·å¯ä»¥ç›´æ¥é€šè¿‡ResourceManage æŸ¥çœ‹åº”ç”¨ç¨‹åºçš„è¿è¡ŒçŠ¶æ€ï¼Œç„¶åå®ƒå°†ä¸ºå„ä¸ªä»»åŠ¡ç”³è¯·èµ„æºï¼Œå¹¶ç›‘æ§å®ƒçš„è¿è¡ŒçŠ¶æ€ï¼Œç›´åˆ°è¿è¡Œç»“æŸï¼Œå³é‡å¤æ­¥éª¤ 4~7ã€‚
4. ApplicationMaster é‡‡ç”¨è½®è¯¢çš„æ–¹å¼é€šè¿‡ RPC åè®®å‘ ResourceManager ç”³è¯·å’Œé¢†å–èµ„æºã€‚
5. ä¸€æ—¦ ApplicationMaster ç”³è¯·åˆ°èµ„æºåï¼Œä¾¿ä¸å¯¹åº”çš„ NodeManager é€šä¿¡ï¼Œè¦æ±‚å®ƒå¯åŠ¨ä»»åŠ¡ã€‚

6. NodeManager ä¸ºä»»åŠ¡è®¾ç½®å¥½è¿è¡Œç¯å¢ƒï¼ˆåŒ…æ‹¬ç¯å¢ƒå˜é‡ã€ JAR åŒ…ã€äºŒè¿›åˆ¶ç¨‹åºç­‰ï¼‰åï¼Œå°†ä»»åŠ¡å¯åŠ¨å‘½ä»¤å†™åˆ°ä¸€ä¸ªè„šæœ¬ä¸­ï¼Œå¹¶é€šè¿‡è¿è¡Œè¯¥è„šæœ¬å¯åŠ¨ä»»åŠ¡ã€‚

7. å„ä¸ªä»»åŠ¡é€šè¿‡æŸä¸ª RPC åè®®å‘ ApplicationMaster æ±‡æŠ¥è‡ªå·±çš„çŠ¶æ€å’Œè¿›åº¦ï¼Œä»¥è®©ApplicationMaster éšæ—¶æŒæ¡å„ä¸ªä»»åŠ¡çš„è¿è¡ŒçŠ¶ æ€ï¼Œä»è€Œå¯ä»¥åœ¨ä»»åŠ¡å¤±è´¥æ—¶é‡æ–°å¯åŠ¨ä»»åŠ¡ã€‚åœ¨åº”ç”¨ç¨‹åºè¿è¡Œè¿‡ç¨‹ä¸­ï¼Œç”¨æˆ·å¯éšæ—¶é€šè¿‡ RPC å‘ ApplicationMaster æŸ¥è¯¢åº”ç”¨ç¨‹åºçš„å½“å‰è¿è¡ŒçŠ¶æ€ã€‚

8. åº”ç”¨ç¨‹åºè¿è¡Œå®Œæˆåï¼Œ ApplicationMaster å‘ ResourceManager æ³¨é”€å¹¶å…³é—­è‡ªå·±ã€‚



#### Hbase

1. æ¶‰åŠåˆ°æ¦‚å¿µï¼Œæ–‡æ¡£



#### Sparkç¯‡

1. sparkåŸç†

   Sparkåº”ç”¨è½¬æ¢æµç¨‹

![image-20210907194907539](https://gitee.com/vicxsl/img/raw/master/img/1631015347974/image-20210907194907539.png)



1. sparkåº”ç”¨æäº¤åï¼Œç»å†äº†ä¸€ç³»åˆ—çš„è½¬æ¢ï¼Œæœ€åæˆä¸ºtaskåœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šæ‰§è¡Œ
2. RDDçš„Actionç®—å­è§¦å‘Jobçš„æäº¤ï¼Œç”ŸæˆRDD DAG
3. ç”±DAGSchedulerå°†RDD DAGè½¬åŒ–ä¸ºStage DAGï¼Œæ¯ä¸ªStageä¸­äº§ç”Ÿç›¸åº”çš„Taské›†åˆ
4. TaskSchedulerå°†ä»»åŠ¡åˆ†å‘åˆ°Executoræ‰§è¡Œ
5. æ¯ä¸ªä»»åŠ¡å¯¹åº”ç›¸åº”çš„ä¸€ä¸ªæ•°æ®å—ï¼Œåªç”¨ç”¨æˆ·å®šä¹‰çš„å‡½æ•°å¤„ç†æ•°æ®å—



**Driverè¿è¡Œåœ¨ Workerä¸Š**

é€šè¿‡ org.apache.spark.deploy.Clientç±»æ‰§è¡Œä½œä¸šï¼Œä½œä¸šè¿è¡Œå‘½ä»¤å¦‚ä¸‹ï¼š

![image-20210907195005475](https://gitee.com/vicxsl/img/raw/master/img/1631015405898/image-20210907195005475.png)

ä½œä¸šæ‰§è¡Œæµç¨‹æè¿°ï¼š

1. å®¢æˆ·ç«¯æäº¤ä½œä¸šç»™ Master
2. Masterè®©ä¸€ä¸ª Workerå¯åŠ¨ Driverï¼Œå³ SchedulerBackendã€‚ Workeråˆ›å»ºä¸€ä¸ª DriverRunnerçº¿ç¨‹ï¼Œ DriverRunnerå¯åŠ¨ SchedulerBackendè¿›ç¨‹ã€‚
3. å¦å¤– Masterè¿˜ä¼šè®©å…¶ä½™ Workerå¯åŠ¨ Exeuctorï¼Œå³ ExecutorBackendã€‚ Workeråˆ›å»ºä¸€ä¸ª ExecutorRunnerçº¿ç¨‹ï¼Œ ExecutorRunnerä¼šå¯åŠ¨ ExecutorBackendè¿›ç¨‹ã€‚
4. ExecutorBackendå¯åŠ¨åä¼šå‘ Driverçš„ SchedulerBackendæ³¨å†Œã€‚ SchedulerBackendè¿›ç¨‹ä¸­åŒ…å« DAGSchedulerå®ƒä¼šæ ¹æ®ç”¨æˆ·ç¨‹åºï¼Œç”Ÿæˆæ‰§è¡Œè®¡åˆ’ï¼Œå¹¶è°ƒåº¦æ‰§è¡Œã€‚å¯¹äºæ¯ä¸ª stageçš„ taskï¼Œéƒ½ä¼šè¢«å­˜æ”¾åˆ° TaskSchedulerä¸­ï¼ŒExecutorBackendå‘ SchedulerBackendæ±‡æŠ¥çš„æ—¶å€™æŠŠ TaskSchedulerä¸­çš„ taskè°ƒåº¦åˆ° ExecutorBackendæ‰§ è¡Œã€‚
5. æ‰€æœ‰ stageéƒ½å®Œæˆåä½œä¸šç»“æŸã€‚



**Driverè¿è¡Œåœ¨å®¢æˆ·ç«¯**

![image-20210907195103244](https://gitee.com/vicxsl/img/raw/master/img/1631015463660/image-20210907195103244.png)

ä½œä¸šæ‰§è¡Œæµç¨‹æè¿°ï¼š

1. å®¢æˆ·ç«¯å¯åŠ¨åç›´æ¥è¿è¡Œç”¨æˆ·ç¨‹åºï¼Œå¯åŠ¨ Driverç›¸å…³çš„å·¥ä½œï¼š DAGSchedulerå’Œ BlockManagerMasterç­‰ã€‚
2. å®¢æˆ·ç«¯çš„ Driverå‘ Masteræ³¨å†Œã€‚
3. Masterè¿˜ä¼šè®© Workerå¯åŠ¨ Exeuctorã€‚ Workeråˆ›å»ºä¸€ä¸ª ExecutorRunnerçº¿ç¨‹ï¼Œ ExecutorRunnerä¼šå¯åŠ¨ExecutorBackendè¿›ç¨‹ã€‚
4. ExecutorBackendå¯åŠ¨åä¼šå‘ Driverçš„ SchedulerBackendæ³¨å†Œã€‚ Driverçš„ DAGSchedulerè§£æä½œä¸šå¹¶ç”Ÿæˆç›¸åº”çš„ Stageï¼Œæ¯ä¸ª StageåŒ…å«çš„ Taské€šè¿‡ TaskScheduleråˆ†é…ç»™ Executoræ‰§è¡Œã€‚
5. æ‰€æœ‰ stageéƒ½å®Œæˆåä½œä¸šç»“æŸã€‚
